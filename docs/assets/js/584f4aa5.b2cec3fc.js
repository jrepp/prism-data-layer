"use strict";(globalThis.webpackChunkdocusaurus=globalThis.webpackChunkdocusaurus||[]).push([[16993],{17856:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>i,metadata:()=>t,toc:()=>o});const t=JSON.parse('{"id":"rfc-014","title":"Layered Data Access Patterns","description":"Abstract","source":"@site/../docs-cms/rfcs/rfc-014-layered-data-access-patterns.md","sourceDirName":".","slug":"/rfc-014","permalink":"/prism-data-layer/rfc/rfc-014","draft":false,"unlisted":false,"editUrl":"https://github.com/jrepp/prism-data-layer/tree/main/docs-cms/../docs-cms/rfcs/rfc-014-layered-data-access-patterns.md","tags":[{"inline":true,"label":"architecture","permalink":"/prism-data-layer/rfc/tags/architecture"},{"inline":true,"label":"patterns","permalink":"/prism-data-layer/rfc/tags/patterns"},{"inline":true,"label":"composition","permalink":"/prism-data-layer/rfc/tags/composition"},{"inline":true,"label":"data-access","permalink":"/prism-data-layer/rfc/tags/data-access"},{"inline":true,"label":"layering","permalink":"/prism-data-layer/rfc/tags/layering"}],"version":"current","frontMatter":{"author":"Platform Team","created":"2025-10-09T00:00:00.000Z","doc_uuid":"b1c3f1d6-8a94-4f78-a954-67b7da3058c3","id":"rfc-014","project_id":"prism-data-layer","status":"Proposed","tags":["architecture","patterns","composition","data-access","layering"],"title":"Layered Data Access Patterns","updated":"2025-10-09T00:00:00.000Z"},"sidebar":"rfcSidebar","previous":{"title":"Neptune Graph Backend Implementation \u2022 RFC-013","permalink":"/prism-data-layer/rfc/rfc-013"},"next":{"title":"Plugin Acceptance Test Framework (Interface-Based Testing) \u2022 RFC-015","permalink":"/prism-data-layer/rfc/rfc-015"}}');var s=a(74848),r=a(28453);const i={author:"Platform Team",created:new Date("2025-10-09T00:00:00.000Z"),doc_uuid:"b1c3f1d6-8a94-4f78-a954-67b7da3058c3",id:"rfc-014",project_id:"prism-data-layer",status:"Proposed",tags:["architecture","patterns","composition","data-access","layering"],title:"Layered Data Access Patterns",updated:new Date("2025-10-09T00:00:00.000Z")},c=void 0,l={},o=[{value:"Abstract",id:"abstract",level:2},{value:"Motivation",id:"motivation",level:2},{value:"Pattern 2: Large Payload Pub/Sub (Claim Check)",id:"pattern-2-large-payload-pubsub-claim-check",level:3},{value:"Pattern 4: Change Data Capture with Kafka (CDC + Outbox)",id:"pattern-4-change-data-capture-with-kafka-cdc--outbox",level:3},{value:"Pattern 6: Cached Reads with Auto-Invalidation (Cache + CDC)",id:"pattern-6-cached-reads-with-auto-invalidation-cache--cdc",level:3},{value:"Building on Existing RFCs",id:"building-on-existing-rfcs",level:2},{value:"RFC-001: Prism Architecture",id:"rfc-001-prism-architecture",level:3},{value:"RFC-002: Data Layer Interface",id:"rfc-002-data-layer-interface",level:3},{value:"RFC-007: Cache Strategies",id:"rfc-007-cache-strategies",level:3},{value:"RFC-009: Distributed Reliability Patterns",id:"rfc-009-distributed-reliability-patterns",level:3},{value:"RFC-010: Admin Protocol with OIDC",id:"rfc-010-admin-protocol-with-oidc",level:3},{value:"RFC-011: Data Proxy Authentication",id:"rfc-011-data-proxy-authentication",level:3},{value:"Configuration Schema",id:"configuration-schema",level:2},{value:"Namespace Pattern Configuration",id:"namespace-pattern-configuration",level:3},{value:"Pattern Validation",id:"pattern-validation",level:3},{value:"Testing Strategy",id:"testing-strategy",level:2},{value:"Unit Tests: Individual Patterns",id:"unit-tests-individual-patterns",level:3},{value:"Integration Tests: Pattern Chains",id:"integration-tests-pattern-chains",level:3},{value:"End-to-End Tests",id:"end-to-end-tests",level:3},{value:"Performance Characteristics",id:"performance-characteristics",level:2},{value:"Pattern Overhead",id:"pattern-overhead",level:3},{value:"Example: Outbox + Claim Check Performance",id:"example-outbox--claim-check-performance",level:3},{value:"Migration Strategy",id:"migration-strategy",level:2},{value:"Phase 1: Single Pattern (Low Risk)",id:"phase-1-single-pattern-low-risk",level:3},{value:"Phase 2: Compose Two Patterns (Medium Risk)",id:"phase-2-compose-two-patterns-medium-risk",level:3},{value:"Phase 3: Complex Composition (Higher Risk)",id:"phase-3-complex-composition-higher-risk",level:3},{value:"Observability",id:"observability",level:2},{value:"Pattern Metrics",id:"pattern-metrics",level:3},{value:"References",id:"references",level:2},{value:"Related RFCs",id:"related-rfcs",level:3},{value:"External References",id:"external-references",level:3},{value:"ADRs",id:"adrs",level:3},{value:"Open Questions",id:"open-questions",level:2},{value:"Revision History",id:"revision-history",level:2}];function d(e){const n={a:"a",br:"br",code:"code",dyn:"dyn",h1:"h1",h2:"h2",h3:"h3",li:"li",offset:"offset",ol:"ol",p:"p",pre:"pre",publishcontext:"publishcontext",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",u8:"u8",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"abstract",children:"Abstract"}),"\n",(0,s.jsxs)(n.p,{children:["This RFC specifies how Prism separates the ",(0,s.jsx)(n.strong,{children:"client API"})," (data access patterns like Queue, PubSub, Reader) from the ",(0,s.jsx)(n.strong,{children:"backend implementation"})," (composed strategies for satisfying those APIs). By layering reliability patterns (Claim Check, Outbox, CDC, Tiered Storage) beneath client-facing interfaces, Prism enables powerful data access capabilities without exposing complexity to applications."]}),"\n",(0,s.jsx)(n.h2,{id:"motivation",children:"Motivation"}),"\n",(0,s.jsx)(n.p,{children:"Modern distributed applications require complex reliability patterns, but implementing them correctly is difficult:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problems"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Applications must implement reliability logic (claim check, outbox, tiered storage) themselves"}),"\n",(0,s.jsx)(n.li,{children:"Backend-specific logic leaks into application code"}),"\n",(0,s.jsx)(n.li,{children:"Switching backends requires rewriting application logic"}),"\n",(0,s.jsx)(n.li,{children:"Patterns (e.g., Claim Check + Pub/Sub) must be composed manually"}),"\n",(0,s.jsx)(n.li,{children:"Testing reliability patterns requires complex infrastructure"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solution"}),": Prism provides a ",(0,s.jsx)(n.strong,{children:"layered architecture"})," that separates concerns:"]}),"\n",(0,s.jsx)(n.p,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Layer 3: Client API (What)                       \u2502\n\u2502  Queue | PubSub | Reader | Transact | Cache             \u2502\n\u2502  "I want to publish messages to a queue"                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502\n\u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Layer 2: Pattern Composition (How)               \u2502\n\u2502  Claim Check | Outbox | CDC | Tiered Storage | WAL      \u2502\n\u2502  "Automatically store large payloads in S3"              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502\n\u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Layer 1: Backend Execution (Where)               \u2502\n\u2502  Kafka | NATS | Postgres | Redis | S3 | ClickHouse      \u2502\n\u2502  "Connect to and execute operations on backend"          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Goals:**\n- Define clear separation between client API and backend strategies\n- Document how to compose multiple patterns for a single API\n- Provide configuration-based pattern selection\n- Enable testing patterns independently\n- Support backend migration without client changes\n\n**Non-Goals:**\n- Runtime pattern switching (patterns chosen at namespace creation)\n- Arbitrary pattern composition (only compatible patterns can layer)\n- Application-level customization of patterns (Prism controls implementation)\n\n## Client Pattern Catalog\n\nThis section shows **how application owners request high-level patterns** without needing to understand internal implementation. Each pattern maps to common application problems.\n\n### Pattern 1: Durable Operation Log (WAL - Write-Ahead Log)\n\n**Application Problem:**\nYou need **guaranteed durability** for operations - write operations must be persisted to disk before being acknowledged, and consumers must reliably process and acknowledge each operation. This is critical for financial transactions, order processing, audit logs, or any scenario where **operations cannot be lost**.\n\n**What You Get:**\n- **Durability guarantee**: Operations persisted to disk before acknowledgment\n- **Reliable consumption**: Consumers must explicitly acknowledge each operation\n- **Crash recovery**: Operations survive proxy/consumer crashes\n- **Replay capability**: Re-process operations from any point in the log\n- **Ordered processing**: Operations processed in write order\n\n**Why WAL is Critical for Reliability:**\n\n"})}),"\n",(0,s.jsx)(n.p,{children:"sequenceDiagram\nparticipant App as Application\nparticipant Proxy as Prism Proxy\nparticipant WAL as WAL (Disk)\nparticipant Consumer as Consumer\nparticipant Backend as Backend (Kafka)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Note over App,Backend: Write Path (Durability)\nApp->>Proxy: Publish operation\nProxy->>WAL: 1. Append to WAL (fsync)\nWAL--\x3e>Proxy: 2. Persisted to disk\nProxy--\x3e>App: 3. Acknowledge (operation durable)\n\nNote over Proxy,Backend: Async flush to backend\nProxy->>Backend: 4. Flush WAL \u2192 Kafka\nBackend--\x3e>Proxy: 5. Kafka acknowledged\n\nNote over Consumer,Backend: Read Path (Reliable Consumption)\nConsumer->>Backend: 6. Consume from Kafka\nBackend--\x3e>Consumer: 7. Operation data\nConsumer->>Consumer: 8. Process operation\nConsumer->>Backend: 9. Acknowledge (committed offset)\n\nNote over App,Backend: Crash Recovery\nProxy->>WAL: On restart: Read uncommitted WAL entries\nProxy->>Backend: Replay to backend\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Client Configuration:**\n"})}),"\n",(0,s.jsx)(n.h1,{id:"declare-what-you-need---prism-handles-the-rest",children:"Declare what you need - Prism handles the rest"}),"\n",(0,s.jsx)(n.p,{children:"namespaces:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"name: order-processing\npattern: durable-queue      # What pattern you need"}),"\n",(0,s.jsx)(n.h1,{id:"tell-prism-what-guarantees-you-need",children:"Tell Prism what guarantees you need"}),"\n",(0,s.jsx)(n.p,{children:"needs:\ndurability: strong        # \u2192 Prism adds WAL with fsync\nreplay: enabled           # \u2192 Prism keeps committed log for replay\nretention: 30days         # \u2192 Prism retains WAL for 30 days\nordered: true             # \u2192 Prism guarantees order"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Client Code (Producer):**\n"})}),"\n",(0,s.jsx)(n.h1,{id:"application-publishes-operation---prism-ensures-durability",children:"Application publishes operation - Prism ensures durability"}),"\n",(0,s.jsx)(n.p,{children:'order = {"order_id": 12345, "amount": 99.99, "status": "pending"}'}),"\n",(0,s.jsx)(n.h1,{id:"operation-is-persisted-to-wal-before-acknowledgment",children:"Operation is persisted to WAL BEFORE acknowledgment"}),"\n",(0,s.jsx)(n.p,{children:'response = client.publish("orders", order)'}),"\n",(0,s.jsx)(n.h1,{id:"at-this-point",children:"At this point:"}),"\n",(0,s.jsx)(n.h1,{id:"-operation-written-to-disk-survived-crash",children:"\u2713 Operation written to disk (survived crash)"}),"\n",(0,s.jsx)(n.h1,{id:"-will-be-delivered-to-consumers",children:"\u2713 Will be delivered to consumers"}),"\n",(0,s.jsx)(n.h1,{id:"-can-be-replayed-if-needed",children:"\u2713 Can be replayed if needed"}),"\n",(0,s.jsx)(n.p,{children:'print("Order persisted:", response.offset)'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Client Code (Consumer):**\n"})}),"\n",(0,s.jsx)(n.h1,{id:"consumer-must-explicitly-acknowledge-each-operation",children:"Consumer must explicitly acknowledge each operation"}),"\n",(0,s.jsx)(n.p,{children:'for operation in client.consume("orders"):\ntry:\n# Process the operation\nresult = process_order(operation.payload)'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"    # MUST acknowledge after successful processing\n    operation.ack()  # Commits offset, operation won't be redelivered\n\nexcept Exception as e:\n    # On failure: don't ack, operation will be retried\n    logging.error(f\"Failed to process order: {e}\")\n    operation.nack()  # Explicit negative ack (immediate retry)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**What Happens Internally:**\n\n1. **Write Path (Producer)**:\n   - **Layer 3**: Client calls `publish()`\n   - **Layer 2**: WAL pattern writes to local append-only log on disk\n   - **Layer 2**: Calls `fsync()` to ensure durability (survives crash)\n   - **Layer 3**: Returns acknowledgment to client\n   - **Background**: WAL flusher asynchronously sends to Kafka\n\n2. **Read Path (Consumer)**:\n   - **Layer 1**: Consumer reads from Kafka\n   - **Layer 2**: WAL pattern tracks consumer position\n   - **Consumer**: Processes operation\n   - **Consumer**: Calls `operation.ack()` to commit progress\n   - **Layer 2**: Updates consumer offset (operation marked consumed)\n\n3. **Crash Recovery**:\n   - **On proxy restart**: Read uncommitted WAL entries from disk\n   - **Layer 2**: Replay uncommitted operations to Kafka\n   - **Guarantee**: Zero lost operations\n\n**Real-World Example:**\nProblem: E-commerce order processing - cannot lose orders\nChallenge: Proxy crashes between accepting order and publishing to Kafka\n\nBefore Prism (without WAL):\n  1. Accept order HTTP request\n  2. Publish to Kafka\n  3. (CRASH HERE) \u2192 Order lost forever\n  4. Return 200 OK to customer\n  Result: Customer charged, order never processed\n\nWith Prism WAL:\n  1. Accept order HTTP request\n  2. Write to WAL on disk (fsync)\n  3. Return 200 OK to customer (order persisted)\n  4. (CRASH HERE) \u2192 WAL entry on disk\n  5. On restart: Replay WAL \u2192 Kafka\n  Result: Zero lost orders, customer trust maintained\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Performance Characteristics:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Write latency: ",(0,s.jsx)(n.strong,{children:"2-5ms"})," (includes fsync to disk)"]}),"\n",(0,s.jsxs)(n.li,{children:["Throughput: ",(0,s.jsx)(n.strong,{children:"10k-50k operations/sec"})," per proxy instance"]}),"\n",(0,s.jsxs)(n.li,{children:["Durability: ",(0,s.jsx)(n.strong,{children:"Survives crashes, power loss"})]}),"\n",(0,s.jsxs)(n.li,{children:["Trade-off: Slightly higher latency vs in-memory queue, but ",(0,s.jsx)(n.strong,{children:"zero data loss"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"pattern-2-large-payload-pubsub-claim-check",children:"Pattern 2: Large Payload Pub/Sub (Claim Check)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Application Problem:"}),"\nYou need to publish large files (videos, ML models, datasets) but message queues have size limits (Kafka: 10MB, NATS: 8MB)."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"What You Get:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Publish files up to 5GB through standard ",(0,s.jsx)(n.code,{children:"publish()"})," API"]}),"\n",(0,s.jsx)(n.li,{children:"Automatic storage management (S3/Blob)"}),"\n",(0,s.jsx)(n.li,{children:"Transparent retrieval for subscribers"}),"\n",(0,s.jsx)(n.li,{children:"Automatic cleanup after consumption"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Client Configuration:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"namespaces:\n  - name: video-processing\n    pattern: pubsub\n\n    needs:\n      max_message_size: 5GB        # \u2192 Prism adds Claim Check\n      storage_backend: s3          # \u2192 Prism configures S3 storage\n      cleanup: on_consume          # \u2192 Prism auto-deletes after read\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Client Code:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'# Publisher: Send large video file (no special handling)\nvideo_bytes = open("movie.mp4", "rb").read()  # 2.5GB\nclient.publish("videos", video_bytes)\n# Prism: Detects size > threshold, stores in S3, publishes reference\n\n# Subscriber: Receive full video (transparent retrieval)\nevent = client.subscribe("videos")\nvideo_bytes = event.payload  # Full 2.5GB reconstructed\nprocess_video(video_bytes)\n# Prism: Fetches from S3, deletes S3 object after consumption\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"What Happens Internally:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Layer 3"}),": Client calls ",(0,s.jsx)(n.code,{children:"publish(2.5GB payload)"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Layer 2"}),": Claim Check detects size > 1MB threshold"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Layer 2"}),": Stores payload in S3, generates claim check ID"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Layer 1"}),": Publishes lightweight message to Kafka (just metadata)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Real-World Example:"}),"\nProblem: ML team publishes 50GB model weights per training run\nBefore Prism: Manual S3 upload + manual message with S3 key\nWith Prism: Standard publish() API, Prism handles everything"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n### Pattern 3: Transactional Messaging (Outbox)\n\n**Application Problem:**\nYou need guaranteed message delivery when updating database - no lost messages, no duplicates, even if Kafka is down.\n\n**What You Get:**\n- Atomic database update + message publish\n- Guaranteed delivery (survives crashes, Kafka outages)\n- Exactly-once semantics\n- No dual-write problems\n\n**Client Configuration:**\n"})}),"\n",(0,s.jsx)(n.p,{children:"namespaces:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"name: order-processing\npattern: queue"}),"\n",(0,s.jsx)(n.p,{children:"needs:\nconsistency: strong          # \u2192 Prism adds Outbox pattern\ndelivery_guarantee: exactly_once"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Client Code:**\n"})}),"\n",(0,s.jsx)(n.h1,{id:"application-code-atomically-update-db-and-publish-event",children:"Application code: Atomically update DB and publish event"}),"\n",(0,s.jsx)(n.p,{children:"with client.transaction() as tx:\n# 1. Update database\ntx.execute(\"UPDATE orders SET status='completed' WHERE id=$1\", order_id)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'# 2. Publish event (atomic with DB update)\ntx.publish("order-events", {"order_id": order_id, "status": "completed"})\n\n# If commit succeeds, event WILL be published eventually\n# If commit fails, event is NOT published\ntx.commit()\n'})}),"\n",(0,s.jsx)(n.h1,{id:"prism-handles-background-publishing-from-outbox-table",children:"Prism handles background publishing from outbox table"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**What Happens Internally:**\n1. **Layer 3**: Client calls `tx.publish()`\n2. **Layer 2**: Outbox pattern inserts into `outbox` table (same transaction)\n3. **Layer 1**: Transaction commits to Postgres\n4. **Background**: Outbox publisher polls table, publishes to Kafka, marks published\n\n**Real-World Example:**\nProblem: E-commerce order completion must trigger notification\nBefore Prism: Dual write bug caused missed notifications\nWith Prism: Outbox pattern guarantees delivery\n"})}),"\n",(0,s.jsx)(n.h3,{id:"pattern-4-change-data-capture-with-kafka-cdc--outbox",children:"Pattern 4: Change Data Capture with Kafka (CDC + Outbox)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Application Problem:"}),"\nYou need to stream database changes to other systems (cache, search index, analytics) without dual writes."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"What You Get:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Automatic capture of database changes (INSERT/UPDATE/DELETE)"}),"\n",(0,s.jsx)(n.li,{children:"Stream changes to Kafka for consumption"}),"\n",(0,s.jsx)(n.li,{children:"No application code changes required"}),"\n",(0,s.jsx)(n.li,{children:"Guaranteed ordering per key"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Client Configuration:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"namespaces:\n  - name: user-profiles\n    pattern: reader  # Normal database reads/writes\n\n    # Enable CDC streaming\n    cdc:\n      enabled: true\n      source: postgres             # \u2192 Prism captures PostgreSQL WAL\n      destination: kafka           # \u2192 Prism streams to Kafka\n      topic: user-profile-changes\n\n      # What to capture\n      tables: [user_profiles]\n      operations: [INSERT, UPDATE, DELETE]\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Client Code:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'# Application: Normal database operations (no CDC code!)\nclient.update("user_profiles", user_id, {"email": "new@email.com"})\n\n# Prism automatically publishes CDC event to Kafka:\n# {\n#   "operation": "UPDATE",\n#   "table": "user_profiles",\n#   "before": {"email": "old@email.com"},\n#   "after": {"email": "new@email.com"},\n#   "timestamp": "2025-10-09T10:30:00Z"\n# }\n\n# Other systems consume from Kafka:\ndef cache_invalidator():\n    for change in kafka.consume("user-profile-changes"):\n        if change.operation in ["UPDATE", "DELETE"]:\n            redis.delete(f"user:{change.after.id}:profile")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"What Happens Internally:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Layer 3"}),": Client calls ",(0,s.jsx)(n.code,{children:"update()"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Layer 1"}),": Postgres executes UPDATE"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Layer 2"}),": CDC pattern captures WAL entry"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Layer 2"}),": Transforms WAL \u2192 CDC event"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Layer 1"}),": Publishes to Kafka topic"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Real-World Example:"}),"\nProblem: Keep Elasticsearch search index synced with PostgreSQL\nBefore Prism: Dual write (update DB, update ES) - race conditions\nWith Prism: CDC automatically streams changes, ES consumes"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n### Pattern 5: Transactional Large Payloads (Outbox + Claim Check)\n\n**Application Problem:**\nYou need BOTH transactional guarantees (outbox) AND large payload support (claim check) - ML model releases, video uploads with metadata.\n\n**What You Get:**\n- Atomic transaction (if commit succeeds, event will be published)\n- Large payload support (up to 5GB)\n- No Kafka/NATS size limits\n- Exactly-once delivery\n\n**Client Configuration:**\n"})}),"\n",(0,s.jsx)(n.p,{children:"namespaces:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"name: ml-model-releases\npattern: pubsub"}),"\n",(0,s.jsx)(n.p,{children:"needs:\nconsistency: strong           # \u2192 Prism adds Outbox\nmax_message_size: 5GB        # \u2192 Prism adds Claim Check\ndelivery_guarantee: exactly_once"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Client Code:**\n"})}),"\n",(0,s.jsx)(n.h1,{id:"application-publish-large-model-with-transactional-guarantee",children:"Application: Publish large model with transactional guarantee"}),"\n",(0,s.jsx)(n.p,{children:'model_weights = load_model("model-v2.weights")  # 2GB'}),"\n",(0,s.jsx)(n.p,{children:'with client.transaction() as tx:\n# Update model registry\ntx.execute("""\nINSERT INTO model_registry (name, version, status)\nVALUES ($1, $2, \'published\')\n""", "my-model", "v2")'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'# Publish model (2GB payload, transactional)\ntx.publish("model-releases", {\n    "model_name": "my-model",\n    "version": "v2",\n    "weights": model_weights  # 2GB\n})\n\ntx.commit()\n'})}),"\n",(0,s.jsx)(n.h1,{id:"if-commit-succeeds-model-will-be-published",children:"If commit succeeds: model will be published"}),"\n",(0,s.jsx)(n.h1,{id:"if-commit-fails-s3-object-is-cleaned-up-no-message-sent",children:"If commit fails: S3 object is cleaned up, no message sent"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**What Happens Internally:**\n1. **Layer 3**: Client calls `tx.publish(2GB)`\n2. **Layer 2**: Claim Check stores 2GB in S3\n3. **Layer 2**: Outbox inserts `{claim_check_id}` into outbox table\n4. **Layer 1**: Transaction commits to Postgres\n5. **Background**: Outbox publisher sends lightweight Kafka message\n\n**Real-World Example:**\nProblem: ML platform releases 50GB models, needs atomic model registry + notification\nBefore Prism: Manual S3 + outbox implementation, 500 LOC\nWith Prism: Standard transactional API, Prism composes patterns\n"})}),"\n",(0,s.jsx)(n.h3,{id:"pattern-6-cached-reads-with-auto-invalidation-cache--cdc",children:"Pattern 6: Cached Reads with Auto-Invalidation (Cache + CDC)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Application Problem:"}),"\nYou need fast cached reads but cache must stay fresh when database changes."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"What You Get:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Lightning-fast reads (Redis cache)"}),"\n",(0,s.jsx)(n.li,{children:"Automatic cache invalidation on updates"}),"\n",(0,s.jsx)(n.li,{children:"No stale data"}),"\n",(0,s.jsx)(n.li,{children:"No application cache management code"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Client Configuration:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"namespaces:\n  - name: product-catalog\n    pattern: reader\n\n    # Enable caching with CDC invalidation\n    cache:\n      enabled: true\n      backend: redis\n      ttl: 900  # 15 min fallback\n\n    cdc:\n      enabled: true\n      destination: cache_invalidator\n      operations: [UPDATE, DELETE]\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Client Code:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'# Application: Just read - Prism handles caching\nproduct = client.get("products", product_id)\n# First read: Cache miss \u2192 Query Postgres \u2192 Populate cache\n# Subsequent reads: Cache hit \u2192 Return from Redis (sub-ms)\n\n# Another service updates product\nother_service.update("products", product_id, {"price": 29.99})\n# Prism CDC: Detects change, invalidates cache automatically\n\n# Next read: Cache miss (invalidated) \u2192 Fresh data from Postgres\nproduct = client.get("products", product_id)  # Gets updated price\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"What Happens Internally:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Read Path"}),": Client \u2192 Check Redis \u2192 (miss) \u2192 Query Postgres \u2192 Cache in Redis"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Write Path"}),": Update Postgres \u2192 CDC captures change \u2192 Invalidate Redis key"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Next Read"}),": Cache miss \u2192 Fresh data"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Real-World Example:"}),"\nProblem: Product catalog with millions of reads/sec, frequent price updates\nBefore Prism: Manual cache + manual invalidation, stale data bugs\nWith Prism: Declare cache + CDC, Prism handles everything"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n### Pattern Selection Guide\n\n| Use Case | Recommended Pattern | Configuration |\n|----------|-------------------|---------------|\n| **High-volume logging** | WAL + Tiered Storage | `write_rps: 100k+`, `retention: 90days` |\n| **Large files (videos, models)** | Claim Check | `max_message_size: >1MB` |\n| **Transactional events** | Outbox | `consistency: strong` |\n| **Database change streaming** | CDC | `cdc.enabled: true` |\n| **Large + transactional** | Outbox + Claim Check | Both requirements |\n| **Fast cached reads** | Cache + CDC | `cache.enabled: true`, `cdc.enabled: true` |\n| **Event sourcing** | WAL + Event Store | `audit: true`, `replay: enabled` |\n\n### How Prism Selects Patterns\n\nApplication owners declare **requirements**, Prism selects **patterns**:\n\n"})}),"\n",(0,s.jsx)(n.h1,{id:"application-declares-what-they-need",children:'Application declares "what" they need'}),"\n",(0,s.jsx)(n.p,{children:"namespaces:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"name: video-uploads\nneeds:\nwrite_rps: 5000                    # High throughput\nmax_message_size: 5GB              # Large payloads\nconsistency: strong                # Transactional\nretention: 30days                  # Long-term storage"}),"\n"]}),"\n",(0,s.jsx)(n.h1,{id:"prism-generates-how-to-implement-it",children:'Prism generates "how" to implement it'}),"\n",(0,s.jsx)(n.h1,{id:"internally-translates-to",children:"Internally translates to:"}),"\n",(0,s.jsx)(n.h1,{id:"patterns-wal-outbox-claim-check-tiered-storage",children:"patterns: [WAL, Outbox, Claim Check, Tiered Storage]"}),"\n",(0,s.jsx)(n.h1,{id:"backend-kafka-s3-postgres",children:"backend: [Kafka, S3, Postgres]"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\nApplication owners **never write pattern composition logic** - they declare needs, Prism handles the rest.\n\n## Architecture Overview\n\n### Proxy Internal Structure\n\nThe Prism proxy is structured to cleanly separate concerns across layers:\n\n"})}),"\n",(0,s.jsx)(n.p,{children:'graph TB\nsubgraph "External"\nClient[Client Application]\nend'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'subgraph "Prism Proxy"\n    subgraph "Frontend Layer"\n        gRPC[gRPC Server<br/>:8980]\n        Auth[Authentication<br/>Middleware]\n        SessionMgr[Session<br/>Manager]\n    end\n\n    subgraph "API Layer (Layer 3)"\n        QueueAPI[Queue API]\n        PubSubAPI[PubSub API]\n        ReaderAPI[Reader API]\n        TransactAPI[Transact API]\n    end\n\n    subgraph "Pattern Layer (Layer 2)"\n        direction LR\n        PatternChain[Pattern Chain<br/>Executor]\n\n        subgraph "Patterns"\n            Outbox[Outbox<br/>Pattern]\n            ClaimCheck[Claim Check<br/>Pattern]\n            CDC[CDC<br/>Pattern]\n            Tiered[Tiered Storage<br/>Pattern]\n            WAL[WAL<br/>Pattern]\n        end\n    end\n\n    subgraph "Backend Layer (Layer 1)"\n        BackendRouter[Backend<br/>Router]\n\n        subgraph "Backend Connectors"\n            KafkaConn[Kafka<br/>Connector]\n            PGConn[PostgreSQL<br/>Connector]\n            S3Conn[S3<br/>Connector]\n            RedisConn[Redis<br/>Connector]\n        end\n    end\n\n    subgraph "Observability"\n        Metrics[Prometheus<br/>Metrics]\n        Traces[OpenTelemetry<br/>Traces]\n        Logs[Structured<br/>Logs]\n    end\nend\n\nsubgraph "Backends"\n    Kafka[(Kafka)]\n    Postgres[(PostgreSQL)]\n    S3[(S3)]\n    Redis[(Redis)]\nend\n\nClient --\x3e|mTLS/JWT| gRPC\ngRPC --\x3e Auth\nAuth --\x3e|Validate| SessionMgr\nSessionMgr --\x3e QueueAPI\nSessionMgr --\x3e PubSubAPI\nSessionMgr --\x3e ReaderAPI\nSessionMgr --\x3e TransactAPI\n\nQueueAPI --\x3e PatternChain\nPubSubAPI --\x3e PatternChain\nReaderAPI --\x3e PatternChain\nTransactAPI --\x3e PatternChain\n\nPatternChain --\x3e|Execute| Outbox\nPatternChain --\x3e|Execute| ClaimCheck\nPatternChain --\x3e|Execute| CDC\nPatternChain --\x3e|Execute| Tiered\nPatternChain --\x3e|Execute| WAL\n\nOutbox --\x3e BackendRouter\nClaimCheck --\x3e BackendRouter\nCDC --\x3e BackendRouter\nTiered --\x3e BackendRouter\nWAL --\x3e BackendRouter\n\nBackendRouter --\x3e|Route| KafkaConn\nBackendRouter --\x3e|Route| PGConn\nBackendRouter --\x3e|Route| S3Conn\nBackendRouter --\x3e|Route| RedisConn\n\nKafkaConn --\x3e Kafka\nPGConn --\x3e Postgres\nS3Conn --\x3e S3\nRedisConn --\x3e Redis\n\nPatternChain -.->|Emit| Metrics\nPatternChain -.->|Emit| Traces\nPatternChain -.->|Emit| Logs\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n### Authentication and Authorization Flow\n\n"})}),"\n",(0,s.jsx)(n.p,{children:"sequenceDiagram\nparticipant Client\nparticipant gRPC as gRPC Server\nparticipant Auth as Auth Middleware\nparticipant JWT as JWT Validator\nparticipant RBAC as RBAC Policy Engine\nparticipant Session as Session Manager\nparticipant API as API Layer"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Client->>gRPC: Request + JWT Token\ngRPC->>Auth: Intercept Request\n\nAuth->>JWT: Validate Token\nJWT->>JWT: Check signature<br/>Check expiry<br/>Extract claims\n\nalt Token Valid\n    JWT--\x3e>Auth: Claims {user, groups, scopes}\n\n    Auth->>RBAC: Authorize Operation\n    RBAC->>RBAC: Check namespace access<br/>Check operation permission<br/>Check rate limits\n\n    alt Authorized\n        RBAC--\x3e>Auth: Allow\n        Auth->>Session: Get/Create Session\n        Session--\x3e>Auth: Session Context\n        Auth->>API: Forward Request + Context\n        API--\x3e>Client: Response\n    else Not Authorized\n        RBAC--\x3e>Auth: Deny\n        Auth--\x3e>Client: PermissionDenied (7)\n    end\nelse Token Invalid\n    JWT--\x3e>Auth: Error\n    Auth--\x3e>Client: Unauthenticated (16)\nend\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n### Pattern Layer Execution Flow\n\n"})}),"\n",(0,s.jsxs)(n.p,{children:["sequenceDiagram\nparticipant API as API Layer (Layer 3)\nparticipant Chain as Pattern Chain\nparticipant P1 as Pattern 1",(0,s.jsx)(n.br,{}),"(Outbox)\nparticipant P2 as Pattern 2",(0,s.jsx)(n.br,{}),"(Claim Check)\nparticipant Backend as Backend Layer (Layer 1)\nparticipant Obs as Observability"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'Note over API,Obs: Publish Flow\n\nAPI->>Chain: Publish(topic, payload, metadata)\nChain->>Obs: Start Trace "publish"\n\nChain->>P1: process_publish(ctx)\nP1->>Obs: Span "outbox-pattern"\nP1->>P1: BEGIN TRANSACTION\nP1->>P1: ctx = wrap in outbox\nP1->>Obs: Metric: outbox_inserted++\n\nP1->>P2: Continue with modified ctx\nP2->>Obs: Span "claim-check-pattern"\n\nalt Payload > Threshold\n    P2->>Backend: Store payload in S3\n    Backend--\x3e>P2: S3 URL\n    P2->>P2: Replace payload with<br/>claim_check_id\n    P2->>Obs: Metric: claim_check_stored++\nend\n\nP2->>P1: Return modified ctx\nP1->>Backend: INSERT INTO outbox\nP1->>P1: COMMIT TRANSACTION\n\nP1->>Chain: Success\nChain->>Obs: End Trace (duration: 52ms)\nChain->>API: PublishResponse\n\nNote over API,Obs: Background: Outbox Publisher\n\nloop Every 100ms\n    P1->>Backend: SELECT unpublished FROM outbox\n    P1->>Backend: Publish to Kafka\n    P1->>Backend: UPDATE outbox published_at\n    P1->>Obs: Metric: outbox_published++\nend\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n### Pattern Routing and Backend Execution\n\n"})}),"\n",(0,s.jsxs)(n.p,{children:['graph LR\nsubgraph "Pattern Layer"\nInput[Pattern Input',(0,s.jsx)(n.br,{}),"Context]"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'    subgraph "Pattern Decision Tree"\n        CheckOutbox{Outbox<br/>Enabled?}\n        CheckClaim{Claim Check<br/>Enabled?}\n        CheckSize{Payload > 1MB?}\n        CheckCDC{CDC<br/>Enabled?}\n    end\n\n    Output[Pattern Output<br/>Context]\nend\n\nsubgraph "Backend Router"\n    Route[Route by<br/>Backend Type]\n\n    subgraph "Execution Strategies"\n        Direct[Direct Execute]\n        Transact[Transactional<br/>Execute]\n        Stream[Streaming<br/>Execute]\n        Batch[Batch<br/>Execute]\n    end\nend\n\nsubgraph "Backend Connectors"\n    KafkaOps[Kafka Operations]\n    PGOps[Postgres Operations]\n    S3Ops[S3 Operations]\n    RedisOps[Redis Operations]\nend\n\nInput --\x3e CheckOutbox\n\nCheckOutbox --\x3e|Yes| Transact\nCheckOutbox --\x3e|No| CheckClaim\n\nCheckClaim --\x3e|Yes| CheckSize\nCheckClaim --\x3e|No| CheckCDC\n\nCheckSize --\x3e|Yes| S3Ops\nCheckSize --\x3e|No| Output\n\nCheckCDC --\x3e|Yes| KafkaOps\nCheckCDC --\x3e|No| Output\n\nOutput --\x3e Route\n\nRoute --\x3e|Queue/PubSub| Direct\nRoute --\x3e|Transact| Transact\nRoute --\x3e|Reader| Stream\nRoute --\x3e|Bulk Insert| Batch\n\nDirect --\x3e KafkaOps\nTransact --\x3e PGOps\nStream --\x3e PGOps\nBatch --\x3e RedisOps\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n### Three-Layer Model\n\n#### Layer 3: Client API (Abstraction)\n\nThe **What** layer - defines the interface applications use:\n\n"})}),"\n",(0,s.jsx)(n.p,{children:"// Example: PubSub Service\nservice PubSubService {\nrpc Publish(PublishRequest) returns (PublishResponse);\nrpc Subscribe(SubscribeRequest) returns (stream Event);\n}"}),"\n",(0,s.jsx)(n.p,{children:"message PublishRequest {\nstring topic = 1;\nbytes payload = 2;  // Application doesn't know about Claim Check\nmap<string, string> metadata = 3;\n}"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Key Characteristics:**\n- Backend-agnostic (no Kafka/NATS specific details)\n- Pattern-agnostic (no Claim Check/Outbox details)\n- Stable API (evolves slowly)\n- Type-safe via protobuf\n\n#### Layer 2: Pattern Composition (Strategy)\n\nThe **How** layer - implements reliability patterns transparently:\n\n"})}),"\n",(0,s.jsx)(n.h1,{id:"namespace-configuration",children:"Namespace configuration"}),"\n",(0,s.jsx)(n.p,{children:"namespaces:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"name: video-processing"}),"\n",(0,s.jsx)(n.h1,{id:"layer-3-client-sees-pubsub-api",children:"Layer 3: Client sees PubSub API"}),"\n",(0,s.jsx)(n.p,{children:"client_api: pubsub"}),"\n",(0,s.jsx)(n.h1,{id:"layer-2-composed-patterns-order-matters",children:"Layer 2: Composed patterns (order matters!)"}),"\n",(0,s.jsx)(n.p,{children:"patterns:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"type: claim-check        # Pattern 1: Handle large payloads\nthreshold: 1MB\nstorage: s3\nbucket: video-processing"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"type: outbox             # Pattern 2: Transactional guarantees\ntable: video_outbox\ndatabase: postgres"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h1,{id:"layer-1-backend-execution",children:"Layer 1: Backend execution"}),"\n",(0,s.jsx)(n.p,{children:"backend:\nqueue: kafka\ntopic_prefix: video"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Pattern Execution Order:**\n\n"})}),"\n",(0,s.jsx)(n.p,{children:"sequenceDiagram\nparticipant App as Application\nparticipant API as Layer 3: PubSub API\nparticipant Pat1 as Layer 2: Claim Check\nparticipant Pat2 as Layer 2: Outbox\nparticipant Backend as Layer 1: Kafka"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"App->>API: Publish(topic, 50MB payload)\nAPI->>Pat1: Process (50MB > 1MB threshold)\nPat1->>Pat1: Store payload in S3\nPat1->>Pat1: Replace payload with claim_check_id\nPat1->>Pat2: Continue ({topic, claim_check_id})\n\nPat2->>Pat2: INSERT INTO video_outbox<br/>(topic, claim_check_id)\nPat2->>Pat2: COMMIT transaction\n\nPat2->>Backend: Publish to Kafka<br/>(lightweight message)\nBackend--\x3e>Pat2: Acknowledged\nPat2--\x3e>API: Success\nAPI--\x3e>App: PublishResponse\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n#### Layer 1: Backend Execution (Implementation)\n\nThe **Where** layer - connects to and executes on specific backends:\n\n"})}),"\n",(0,s.jsxs)(n.p,{children:["// Backend-specific implementation\nimpl KafkaBackend {\nasync fn publish(&self, topic: &str, payload: &[u8]) -> Result",(0,s.jsx)(n.offset,{children:" {\nself.producer\n.send(topic, payload, None)\n.await\n.map_err(|e| Error::Backend(e))\n}\n}"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Key Characteristics:**\n- Backend-specific logic encapsulated\n- Connection pooling and retries\n- Performance optimization per backend\n- Pluggable (new backends without API changes)\n\n## Pattern Composition\n\n### Compatible Pattern Combinations\n\nNot all patterns can be layered together. Compatibility depends on:\n- **Ordering**: Some patterns must come before others\n- **Data Flow**: Patterns must pass compatible data structures\n- **Semantics**: Patterns can't contradict (e.g., eventual + strong consistency)\n\n#### Composition Matrix\n\n| Base API | Compatible Patterns (In Order) | Example Use Case |\n|----------|-------------------------------|------------------|\n| **PubSub** | Claim Check \u2192 Kafka/NATS | Large payload pub/sub |\n| **PubSub** | Outbox \u2192 Claim Check \u2192 Kafka | Transactional large payloads |\n| **Queue** | Claim Check \u2192 Kafka | Large message queue |\n| **Queue** | WAL \u2192 Tiered Storage | Fast writes + archival |\n| **Reader** | Cache (Look-Aside) \u2192 Postgres | Frequent reads |\n| **Reader** | CDC \u2192 Cache Invalidation | Fresh cached reads |\n| **Transact** | Outbox \u2192 Queue Publisher | Transactional messaging |\n| **Transact** | Event Sourcing \u2192 Materialized Views | Audit + performance |\n\n### Publisher with Claim Check Pattern\n\n**Scenario**: Application needs to publish large video files (50MB-5GB) to a pub/sub system, but Kafka/NATS have 1-10MB message limits.\n\n#### Without Layering (Application Code)\n\n"})}),"\n",(0,s.jsx)(n.h1,{id:"application-must-implement-claim-check-manually",children:"Application must implement Claim Check manually"}),"\n",(0,s.jsx)(n.p,{children:'def publish_video(video_id, video_bytes):\nif len(video_bytes) > 1_000_000:  # > 1MB\n# Upload to S3\ns3_key = f"videos/{video_id}"\ns3.put_object(Bucket="videos", Key=s3_key, Body=video_bytes)'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'    # Publish reference\n    kafka.produce("videos", {\n        "video_id": video_id,\n        "s3_reference": s3_key,\n        "size": len(video_bytes)\n    })\nelse:\n    # Publish inline\n    kafka.produce("videos", {\n        "video_id": video_id,\n        "payload": video_bytes\n    })\n'})}),"\n",(0,s.jsx)(n.h1,{id:"consumer-must-implement-claim-check-retrieval",children:"Consumer must implement Claim Check retrieval"}),"\n",(0,s.jsx)(n.p,{children:'def consume_video():\nmsg = kafka.consume("videos")\nif "s3_reference" in msg:\n# Download from S3\nvideo_bytes = s3.get_object(\nBucket="videos",\nKey=msg["s3_reference"]\n)["Body"].read()\nelse:\nvideo_bytes = msg["payload"]'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"process_video(video_bytes)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Problems**:\n- 20+ lines of boilerplate per producer/consumer\n- Must handle S3 credentials, retries, errors\n- No automatic cleanup of claim check objects\n- Different logic for small vs large payloads\n\n#### With Prism Layering (Zero Application Code)\n\n**Configuration**:\n"})}),"\n",(0,s.jsx)(n.p,{children:"namespaces:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"name: video-processing\nclient_api: pubsub"}),"\n",(0,s.jsx)(n.p,{children:"patterns:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"type: claim-check\nthreshold: 1MB\nstorage:\nbackend: s3\nbucket: prism-claim-checks\nprefix: videos/\ncleanup:\nstrategy: on_consume\nttl: 604800  # 7 days fallback"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"backend:\ntype: kafka\nbrokers: [kafka-1:9092, kafka-2:9092]\ntopic: videos"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Application Code**:\n"})}),"\n",(0,s.jsx)(n.h1,{id:"producer-prism-handles-claim-check-automatically",children:"Producer: Prism handles Claim Check automatically"}),"\n",(0,s.jsx)(n.p,{children:'client.publish("videos", video_bytes)'}),"\n",(0,s.jsx)(n.h1,{id:"prism",children:"Prism:"}),"\n",(0,s.jsx)(n.h1,{id:"1-detects-size--1mb",children:"1. Detects size > 1MB"}),"\n",(0,s.jsx)(n.h1,{id:"2-uploads-to-s3-s3prism-claim-checksvideosuuid",children:"2. Uploads to S3: s3://prism-claim-checks/videos/{uuid}"}),"\n",(0,s.jsx)(n.h1,{id:"3-publishes-kafka-claim_check_id-size-metadata",children:"3. Publishes Kafka: {claim_check_id, size, metadata}"}),"\n",(0,s.jsx)(n.h1,{id:"consumer-prism-reconstructs-full-payload",children:"Consumer: Prism reconstructs full payload"}),"\n",(0,s.jsx)(n.p,{children:'event = client.subscribe("videos")\nvideo_bytes = event.payload  # Prism fetched from S3 automatically\nprocess_video(video_bytes)'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Benefits**:\n- 2 lines of application code (vs 20+)\n- Automatic threshold detection\n- Transparent S3 upload/download\n- Automatic cleanup after consumption\n- Same API for small and large payloads\n\n### Outbox + Claim Check Layering\n\n**Scenario**: Application needs **transactional guarantees** (outbox) AND **large payload handling** (claim check).\n\n#### Pattern Layering\n\n"})}),"\n",(0,s.jsx)(n.p,{children:"sequenceDiagram\nparticipant App as Application\nparticipant Prism as Prism Proxy\nparticipant DB as PostgreSQL\nparticipant S3 as S3 Storage\nparticipant Kafka as Kafka"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"App->>Prism: Publish(topic, 100MB model weights)\n\nNote over Prism: Layer 2: Claim Check Pattern\nPrism->>Prism: Detect 100MB > 1MB threshold\nPrism->>S3: PUT ml-models/model-v2.bin\nS3--\x3e>Prism: Success, S3 URL\n\nNote over Prism: Layer 2: Outbox Pattern\nPrism->>DB: BEGIN TRANSACTION\nPrism->>DB: INSERT INTO outbox<br/>(event_type, claim_check_id,<br/> metadata, payload_size)\nPrism->>DB: COMMIT TRANSACTION\nDB--\x3e>Prism: Transaction committed\n\nPrism--\x3e>App: PublishResponse (success)\n\nNote over Prism: Background: Outbox Publisher\nloop Every 100ms\n    Prism->>DB: SELECT * FROM outbox<br/>WHERE published_at IS NULL\n    DB--\x3e>Prism: Unpublished events\n\n    Prism->>Kafka: Publish lightweight message<br/>{claim_check_id, metadata}\n    Kafka--\x3e>Prism: Acknowledged\n\n    Prism->>DB: UPDATE outbox<br/>SET published_at = NOW()\nend\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Configuration**:\n"})}),"\n",(0,s.jsx)(n.p,{children:"namespaces:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"name: ml-model-releases\nclient_api: pubsub"}),"\n",(0,s.jsx)(n.p,{children:"patterns:"}),"\n",(0,s.jsx)(n.h1,{id:"order-matters-outbox-runs-first-wraps-everything-in-transaction",children:"Order matters: Outbox runs first (wraps everything in transaction)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"type: outbox\ndatabase: postgres\ntable: ml_model_outbox\npublisher:\ninterval: 100ms\nbatch_size: 100"}),"\n"]}),"\n",(0,s.jsx)(n.h1,{id:"claim-check-runs-second-inside-outbox-transaction",children:"Claim Check runs second (inside outbox transaction)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"type: claim-check\nthreshold: 10MB\nstorage:\nbackend: s3\nbucket: ml-models\nprefix: releases/\nmetadata_field: claim_check_id  # Store S3 reference in outbox"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"backend:\ntype: kafka\ntopic: model-releases"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Guarantees**:\n- \u2705 If transaction commits, event WILL be published (outbox)\n- \u2705 If transaction fails, S3 object can be garbage collected\n- \u2705 Large models don't block database (claim check)\n- \u2705 Kafka receives lightweight messages (&lt;1KB)\n\n### CDC + Cache Invalidation Layering\n\n**Scenario**: Keep cache synchronized with database changes using CDC.\n\n#### Pattern Composition\n\n"})}),"\n",(0,s.jsx)(n.p,{children:"namespaces:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"name: user-profiles\nclient_api: reader"}),"\n",(0,s.jsx)(n.p,{children:"patterns:"}),"\n",(0,s.jsx)(n.h1,{id:"pattern-1-look-aside-cache-fast-reads",children:"Pattern 1: Look-Aside Cache (fast reads)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'type: cache\nstrategy: look-aside\nbackend: redis\nttl: 900  # 15 minutes\nkey_pattern: "user:{id}:profile"'}),"\n"]}),"\n",(0,s.jsx)(n.h1,{id:"pattern-2-cdc-for-cache-invalidation",children:"Pattern 2: CDC for cache invalidation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"type: cdc\nsource:\nbackend: postgres\ndatabase: users_db\ntable: user_profiles\nsink:\nbackend: kafka\ntopic: user-profile-changes"}),"\n",(0,s.jsx)(n.h1,{id:"consumers-cache-invalidator",children:"Consumers: Cache invalidator"}),"\n",(0,s.jsx)(n.p,{children:"consumers:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'name: cache-invalidator\ntype: cache_invalidator\nbackend: redis\noperations: [UPDATE, DELETE]\nkey_extractor: "user:{after.id}:profile"'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"backend:\ntype: postgres\ndatabase: users_db"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Data Flow**:\n\n"})}),"\n",(0,s.jsx)(n.p,{children:"graph LR\nApp[Application]\nPrism[Prism Proxy]\nCache[Redis Cache]\nDB[(PostgreSQL)]\nCDC[CDC Connector]\nKafka[Kafka]\nInvalidator[Cache Invalidator]"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"App --\x3e|Read user profile| Prism\n\nPrism --\x3e|1. Check cache| Cache\nCache -.->|Cache Hit| Prism\n\nPrism --\x3e|2. Query DB<br/>(on miss)| DB\nDB -.->|User data| Prism\nPrism -.->|3. Populate cache| Cache\n\nApp2[Another App] --\x3e|Update profile| DB\nDB --\x3e|WAL stream| CDC\nCDC --\x3e|Parse changes| Kafka\nKafka --\x3e|Subscribe| Invalidator\nInvalidator --\x3e|DEL user:123:profile| Cache\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Benefits**:\n- Applications always read from cache (fast)\n- Cache stays synchronized with database\n- No dual writes or race conditions\n- Automatic invalidation on UPDATE/DELETE\n\n## Separation of Concerns\n\n### Client API vs Backend Strategy\n\n#### Example: Queue Service\n\n**Client API (Layer 3)** - Stable interface:\n"})}),"\n",(0,s.jsx)(n.p,{children:"service QueueService {\nrpc Publish(PublishRequest) returns (PublishResponse);\nrpc Subscribe(SubscribeRequest) returns (stream Message);\n}"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Backend Strategy (Layer 2 + 1)** - Implementation details:\n\n| Strategy Combination | Backends | Use Case |\n|---------------------|----------|----------|\n| Queue (simple) | Kafka | Standard message queue |\n| WAL + Queue | Kafka WAL \u2192 Postgres | Durable + queryable queue |\n| Claim Check + Queue | S3 + Kafka | Large message queue |\n| Outbox + Queue | Postgres outbox \u2192 Kafka | Transactional queue |\n| Tiered Queue | Redis (hot) \u2192 Postgres (warm) \u2192 S3 (cold) | Multi-tier retention |\n\n**Application doesn't know which strategy** - same API for all:\n"})}),"\n",(0,s.jsx)(n.h1,{id:"works-with-any-backend-strategy",children:"Works with ANY backend strategy"}),"\n",(0,s.jsx)(n.p,{children:'client.publish("events", payload)\nmessages = client.subscribe("events")'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n### Pattern Configuration Encapsulation\n\nApplications declare requirements; Prism selects patterns:\n\n"})}),"\n",(0,s.jsx)(n.h1,{id:"application-facing-configuration",children:"Application-facing configuration"}),"\n",(0,s.jsx)(n.p,{children:"namespaces:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"name: video-processing\nneeds:\nmessage_size: 5GB              # Prism adds Claim Check\nconsistency: strong            # Prism adds Outbox\nretention: 30 days             # Prism adds Tiered Storage\nthroughput: 10k msgs/s         # Prism sizes Kafka partitions"}),"\n"]}),"\n",(0,s.jsx)(n.h1,{id:"prism-generates-internal-config",children:"Prism generates internal config:"}),"\n",(0,s.jsx)(n.p,{children:"namespaces:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["name: video-processing\nclient_api: pubsub\npatterns:\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"type: outbox              # For consistency"}),"\n",(0,s.jsx)(n.li,{children:"type: claim-check         # For message_size"}),"\n",(0,s.jsx)(n.li,{children:"type: tiered-storage      # For retention\nbackend:\ntype: kafka\npartitions: 20              # For throughput"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n## Stitching Patterns Together\n\n### Pattern Interfaces\n\nEach pattern implements standard interfaces for composability:\n\n"})}),"\n",(0,s.jsx)(n.p,{children:"/// Pattern trait for composing reliability patterns\n#[async_trait]\npub trait Pattern: Send + Sync {\n/// Process outgoing data (before backend)\nasync fn process_publish(\n&self,\nctx: &mut PublishContext,\n) -> Result<()>;"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"/// Process incoming data (after backend)\nasync fn process_consume(\n    &self,\n    ctx: &mut ConsumeContext,\n) -> Result<()>;\n\n/// Pattern metadata\nfn metadata(&self) -> PatternMetadata;\n"})}),"\n",(0,s.jsx)(n.p,{children:"}"}),"\n",(0,s.jsxs)(n.p,{children:["pub struct PublishContext {\npub topic: String,\npub payload: Vec",(0,s.jsx)(n.u8,{children:",\npub metadata: HashMap<String, String>,\npub backend: BackendType,\n}"})]}),"\n",(0,s.jsxs)(n.p,{children:["pub struct ConsumeContext {\npub message_id: String,\npub payload: Vec",(0,s.jsx)(n.u8,{children:",\npub metadata: HashMap<String, String>,\n}"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n### Example: Claim Check Pattern Implementation\n\n"})}),"\n",(0,s.jsxs)(n.p,{children:["pub struct ClaimCheckPattern {\nthreshold: usize,\nstorage: Arc",(0,s.jsx)(n.dyn,{objectstorage:"",children:",\n}"})]}),"\n",(0,s.jsx)(n.p,{children:'#[async_trait]\nimpl Pattern for ClaimCheckPattern {\nasync fn process_publish(&self, ctx: &mut PublishContext) -> Result<()> {\n// Check threshold\nif ctx.payload.len() > self.threshold {\n// Store in object storage\nlet claim_check_id = Uuid::new_v4().to_string();\nlet key = format!("claim-checks/{}", claim_check_id);'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'        self.storage.put(&key, &ctx.payload).await?;\n\n        // Replace payload with reference\n        ctx.metadata.insert("claim_check_id".to_string(), claim_check_id);\n        ctx.metadata.insert("payload_size".to_string(), ctx.payload.len().to_string());\n        ctx.payload = vec![];  // Empty payload, reference in metadata\n    }\n\n    Ok(())\n}\n\nasync fn process_consume(&self, ctx: &mut ConsumeContext) -> Result<()> {\n    // Check for claim check reference\n    if let Some(claim_check_id) = ctx.metadata.get("claim_check_id") {\n        let key = format!("claim-checks/{}", claim_check_id);\n\n        // Fetch from object storage\n        ctx.payload = self.storage.get(&key).await?;\n\n        // Optional: Delete claim check after consumption\n        // self.storage.delete(&key).await?;\n    }\n\n    Ok(())\n}\n\nfn metadata(&self) -> PatternMetadata {\n    PatternMetadata {\n        name: "claim-check".to_string(),\n        version: "1.0.0".to_string(),\n        compatible_backends: vec![BackendType::Kafka, BackendType::Nats],\n    }\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"}"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n### Pattern Chain Execution\n\nPrism executes patterns in order:\n\n"})}),"\n",(0,s.jsxs)(n.p,{children:["pub struct PatternChain {\npatterns: Vec<Box",(0,s.jsx)(n.dyn,{pattern:"",children:">,\n}"})]}),"\n",(0,s.jsxs)(n.p,{children:["impl PatternChain {\npub async fn process_publish(&self, mut ctx: PublishContext) -> Result",(0,s.jsx)(n.publishcontext,{children:" {\n// Execute patterns in order\nfor pattern in &self.patterns {\npattern.process_publish(&mut ctx).await?;\n}\nOk(ctx)\n}"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"pub async fn process_consume(&self, mut ctx: ConsumeContext) -> Result<ConsumeContext> {\n    // Execute patterns in reverse order for consume\n    for pattern in self.patterns.iter().rev() {\n        pattern.process_consume(&mut ctx).await?;\n    }\n    Ok(ctx)\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:"}"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n**Example execution with Outbox + Claim Check**:\n\nPublish Flow:\n  App \u2192 [Layer 3: PubSub API]\n    \u2192 [Layer 2: Outbox Pattern] (begin transaction)\n      \u2192 [Layer 2: Claim Check Pattern] (store large payload)\n        \u2192 [Layer 1: Kafka Backend] (publish lightweight message)\n      \u2190 (commit transaction)\n    \u2190 (return success)\n  \u2190 PublishResponse\n\nConsume Flow:\n  [Layer 1: Kafka Backend] (receive message)\n    \u2192 [Layer 2: Claim Check Pattern] (fetch from S3)\n      \u2192 [Layer 2: Outbox Pattern] (no-op for consume)\n        \u2192 [Layer 3: PubSub API]\n          \u2192 App (full payload reconstructed)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"building-on-existing-rfcs",children:"Building on Existing RFCs"}),"\n",(0,s.jsx)(n.p,{children:"This RFC builds on and connects:"}),"\n",(0,s.jsx)(n.h3,{id:"rfc-001-prism-architecture",children:"RFC-001: Prism Architecture"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Layer 3 Client API"}),' maps to RFC-001 "Interface Layers" (Queue, PubSub, Reader, Transact)']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Layer 1 Backend Execution"}),' maps to RFC-001 "Container Plugin Model"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Layer 2 Pattern Composition"})," is NEW - enables powerful combinations"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"rfc-002-data-layer-interface",children:"RFC-002: Data Layer Interface"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Client-facing protobuf APIs defined in RFC-002"}),"\n",(0,s.jsx)(n.li,{children:"Applications use stable APIs from RFC-002"}),"\n",(0,s.jsx)(n.li,{children:"Patterns (Layer 2) transparently implement RFC-002 interfaces"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"rfc-007-cache-strategies",children:"RFC-007: Cache Strategies"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Look-Aside and Write-Through caching are ",(0,s.jsx)(n.strong,{children:"patterns"})," (Layer 2)"]}),"\n",(0,s.jsx)(n.li,{children:"Can compose with other patterns (e.g., CDC + Cache Invalidation)"}),"\n",(0,s.jsx)(n.li,{children:"Cache configuration moves from application to namespace config"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"rfc-009-distributed-reliability-patterns",children:"RFC-009: Distributed Reliability Patterns"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"All 7 patterns (Claim Check, Outbox, WAL, CDC, Tiered Storage, Event Sourcing, CQRS) live in Layer 2"}),"\n",(0,s.jsx)(n.li,{children:'Can be composed as shown in RFC-009 "Pattern Composition and Layering" section'}),"\n",(0,s.jsx)(n.li,{children:"This RFC formalizes the layering architecture"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"rfc-010-admin-protocol-with-oidc",children:"RFC-010: Admin Protocol with OIDC"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Admin API operates at Layer 3 (control plane, not data plane)"}),"\n",(0,s.jsx)(n.li,{children:"Patterns configured via admin API"}),"\n",(0,s.jsx)(n.li,{children:"Observability of pattern health exposed via admin API"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"rfc-011-data-proxy-authentication",children:"RFC-011: Data Proxy Authentication"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Authentication happens at Layer 3 (before patterns)"}),"\n",(0,s.jsx)(n.li,{children:"Patterns inherit session context"}),"\n",(0,s.jsx)(n.li,{children:"Backend credentials managed at Layer 1"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"configuration-schema",children:"Configuration Schema"}),"\n",(0,s.jsx)(n.h3,{id:"namespace-pattern-configuration",children:"Namespace Pattern Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"namespaces:\n  - name: video-processing\n\n    # Layer 3: Client API\n    client_api: pubsub\n\n    # Layer 2: Pattern composition (ordered!)\n    patterns:\n      - type: outbox\n        enabled: true\n        config:\n          database: postgres\n          table: video_outbox\n          publisher:\n            interval: 100ms\n            batch_size: 100\n\n      - type: claim-check\n        enabled: true\n        config:\n          threshold: 1MB\n          storage:\n            backend: s3\n            bucket: prism-claim-checks\n            prefix: videos/\n          compression: gzip\n          cleanup:\n            strategy: on_consume\n            ttl: 604800\n\n      - type: tiered-storage\n        enabled: false  # Optional pattern\n\n    # Layer 1: Backend configuration\n    backend:\n      type: kafka\n      brokers: [kafka-1:9092, kafka-2:9092]\n      topic: videos\n      partitions: 10\n      replication: 3\n\n    # Observability\n    observability:\n      trace_patterns: true  # Trace each pattern execution\n      pattern_metrics: true\n"})}),"\n",(0,s.jsx)(n.h3,{id:"pattern-validation",children:"Pattern Validation"}),"\n",(0,s.jsx)(n.p,{children:"Prism validates pattern compatibility at namespace creation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"pub fn validate_pattern_chain(\n    api: ClientApi,\n    patterns: &[PatternConfig],\n    backend: &BackendConfig,\n) -> Result<()> {\n    // Check API compatibility\n    for pattern in patterns {\n        if !pattern.supports_api(&api) {\n            return Err(Error::IncompatiblePattern {\n                pattern: pattern.type_name(),\n                api: api.name(),\n            });\n        }\n    }\n\n    // Check pattern ordering\n    for window in patterns.windows(2) {\n        if !window[1].compatible_with(&window[0]) {\n            return Err(Error::IncompatiblePatternOrder {\n                first: window[0].type_name(),\n                second: window[1].type_name(),\n            });\n        }\n    }\n\n    // Check backend compatibility\n    if !backend.supports_patterns(patterns) {\n        return Err(Error::BackendIncompatibleWithPatterns);\n    }\n\n    Ok(())\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"testing-strategy",children:"Testing Strategy"}),"\n",(0,s.jsx)(n.h3,{id:"unit-tests-individual-patterns",children:"Unit Tests: Individual Patterns"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'#[tokio::test]\nasync fn test_claim_check_pattern_threshold() {\n    let storage = Arc::new(MockObjectStorage::new());\n    let pattern = ClaimCheckPattern {\n        threshold: 1_000_000,  // 1MB\n        storage: storage.clone(),\n    };\n\n    // Small payload - should not trigger claim check\n    let mut ctx = PublishContext {\n        topic: "test".to_string(),\n        payload: vec![0u8; 500_000],  // 500KB\n        ..Default::default()\n    };\n\n    pattern.process_publish(&mut ctx).await.unwrap();\n    assert!(!ctx.metadata.contains_key("claim_check_id"));\n    assert_eq!(ctx.payload.len(), 500_000);\n\n    // Large payload - should trigger claim check\n    let mut ctx = PublishContext {\n        topic: "test".to_string(),\n        payload: vec![0u8; 2_000_000],  // 2MB\n        ..Default::default()\n    };\n\n    pattern.process_publish(&mut ctx).await.unwrap();\n    assert!(ctx.metadata.contains_key("claim_check_id"));\n    assert_eq!(ctx.payload.len(), 0);  // Payload replaced\n    assert_eq!(storage.object_count(), 1);\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"integration-tests-pattern-chains",children:"Integration Tests: Pattern Chains"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'#[tokio::test]\nasync fn test_outbox_claim_check_chain() {\n    let db = setup_test_db().await;\n    let s3 = setup_test_s3().await;\n    let kafka = setup_test_kafka().await;\n\n    let chain = PatternChain::new(vec![\n        Box::new(OutboxPattern::new(db.clone())),\n        Box::new(ClaimCheckPattern::new(1_000_000, s3.clone())),\n    ]);\n\n    // Publish large payload\n    let ctx = PublishContext {\n        topic: "test".to_string(),\n        payload: vec![0u8; 5_000_000],  // 5MB\n        ..Default::default()\n    };\n\n    let ctx = chain.process_publish(ctx).await.unwrap();\n\n    // Verify outbox entry created\n    let outbox_entries = db.query("SELECT * FROM outbox WHERE published_at IS NULL").await.unwrap();\n    assert_eq!(outbox_entries.len(), 1);\n\n    // Verify S3 object stored\n    assert_eq!(s3.object_count(), 1);\n\n    // Verify Kafka message is lightweight\n    assert!(ctx.metadata.contains_key("claim_check_id"));\n    assert_eq!(ctx.payload.len(), 0);\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"end-to-end-tests",children:"End-to-End Tests"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'def test_e2e_large_payload_pubsub():\n    # Setup Prism with Outbox + Claim Check\n    prism = PrismTestServer(config={\n        "namespace": "test",\n        "client_api": "pubsub",\n        "patterns": [\n            {"type": "outbox", "database": "postgres"},\n            {"type": "claim-check", "threshold": "1MB", "storage": "s3"}\n        ],\n        "backend": {"type": "kafka"}\n    })\n\n    client = prism.client()\n\n    # Publish 10MB payload\n    large_payload = b"x" * 10_000_000\n    response = client.publish("test-topic", large_payload)\n    assert response.success\n\n    # Consume and verify full payload reconstructed\n    subscriber = client.subscribe("test-topic")\n    event = next(subscriber)\n    assert event.payload == large_payload\n\n    # Verify S3 object cleaned up after consumption\n    assert prism.s3.object_count() == 0\n'})}),"\n",(0,s.jsx)(n.h2,{id:"performance-characteristics",children:"Performance Characteristics"}),"\n",(0,s.jsx)(n.h3,{id:"pattern-overhead",children:"Pattern Overhead"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Pattern"}),(0,s.jsx)(n.th,{children:"Latency Added"}),(0,s.jsx)(n.th,{children:"Memory Overhead"}),(0,s.jsx)(n.th,{children:"Use When"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"None"})}),(0,s.jsx)(n.td,{children:"0ms"}),(0,s.jsx)(n.td,{children:"0MB"}),(0,s.jsx)(n.td,{children:"Simple use cases"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Claim Check"})}),(0,s.jsx)(n.td,{children:"+10-50ms (S3 upload)"}),(0,s.jsx)(n.td,{children:"~10MB (buffer)"}),(0,s.jsx)(n.td,{children:"Payload > 1MB"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Outbox"})}),(0,s.jsx)(n.td,{children:"+5-10ms (DB write)"}),(0,s.jsx)(n.td,{children:"~1MB (buffer)"}),(0,s.jsx)(n.td,{children:"Need transactions"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"CDC"})}),(0,s.jsx)(n.td,{children:"Background"}),(0,s.jsx)(n.td,{children:"~5MB (replication)"}),(0,s.jsx)(n.td,{children:"Keep systems synced"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Tiered Storage"})}),(0,s.jsx)(n.td,{children:"Variable"}),(0,s.jsx)(n.td,{children:"~10MB (tier metadata)"}),(0,s.jsx)(n.td,{children:"Hot/warm/cold data"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"WAL"})}),(0,s.jsx)(n.td,{children:"+2-5ms (log append)"}),(0,s.jsx)(n.td,{children:"~50MB (WAL buffer)"}),(0,s.jsx)(n.td,{children:"High write throughput"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"example-outbox--claim-check-performance",children:"Example: Outbox + Claim Check Performance"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Baseline (no patterns)"}),": 10ms P99 latency, 10k RPS"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"With Outbox + Claim Check"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Small payloads (<1MB): 15ms P99 (+5ms for outbox), 8k RPS"}),"\n",(0,s.jsx)(n.li,{children:"Large payloads (>1MB): 60ms P99 (+50ms for S3 upload), 1k RPS"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Trade-off"}),": Slightly higher latency for ",(0,s.jsx)(n.strong,{children:"strong guarantees"})," (transactional + large payload support)."]}),"\n",(0,s.jsx)(n.h2,{id:"migration-strategy",children:"Migration Strategy"}),"\n",(0,s.jsx)(n.h3,{id:"phase-1-single-pattern-low-risk",children:"Phase 1: Single Pattern (Low Risk)"}),"\n",(0,s.jsx)(n.p,{children:"Start with one pattern per namespace:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"# Before: Direct Kafka\nbackend: kafka\n\n# After: Add Claim Check only\npatterns:\n  - type: claim-check\n    threshold: 1MB\nbackend: kafka\n"})}),"\n",(0,s.jsx)(n.h3,{id:"phase-2-compose-two-patterns-medium-risk",children:"Phase 2: Compose Two Patterns (Medium Risk)"}),"\n",(0,s.jsx)(n.p,{children:"Add second compatible pattern:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"patterns:\n  - type: outbox         # Transactional guarantees\n  - type: claim-check    # Large payload handling\nbackend: kafka\n"})}),"\n",(0,s.jsx)(n.h3,{id:"phase-3-complex-composition-higher-risk",children:"Phase 3: Complex Composition (Higher Risk)"}),"\n",(0,s.jsx)(n.p,{children:"Layer 3+ patterns for advanced use cases:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"patterns:\n  - type: outbox\n  - type: claim-check\n  - type: tiered-storage  # Archive old messages to S3\n  - type: cdc             # Replicate to analytics DB\nbackend: kafka\n"})}),"\n",(0,s.jsx)(n.h2,{id:"observability",children:"Observability"}),"\n",(0,s.jsx)(n.h3,{id:"pattern-metrics",children:"Pattern Metrics"}),"\n",(0,s.jsx)(n.h1,{id:"claim-check",children:"Claim Check"}),"\n",(0,s.jsx)(n.p,{children:'prism_pattern_claim_check_stored_total{namespace="videos"} 1234\nprism_pattern_claim_check_retrieved_total{namespace="videos"} 1230\nprism_pattern_claim_check_storage_bytes{namespace="videos"} 5.2e9\nprism_pattern_claim_check_cleanup_success{namespace="videos"} 1230'}),"\n",(0,s.jsx)(n.h1,{id:"outbox",children:"Outbox"}),"\n",(0,s.jsx)(n.p,{children:'prism_pattern_outbox_inserted_total{namespace="videos"} 1234\nprism_pattern_outbox_published_total{namespace="videos"} 1234\nprism_pattern_outbox_lag_seconds{namespace="videos"} 0.15\nprism_pattern_outbox_pending_count{namespace="videos"} 5'}),"\n",(0,s.jsx)(n.h1,{id:"pattern-chain",children:"Pattern Chain"}),"\n",(0,s.jsx)(n.p,{children:'prism_pattern_chain_duration_seconds{namespace="videos", pattern="claim-check"} 0.042\nprism_pattern_chain_duration_seconds{namespace="videos", pattern="outbox"} 0.008'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\n### Distributed Tracing\n\nTrace: Publish large video\n\u251c\u2500 Span: PubSubService.Publish [12ms]\n\u2502  \u251c\u2500 Span: OutboxPattern.process_publish [8ms]\n\u2502  \u2502  \u251c\u2500 Span: db.begin_transaction [1ms]\n\u2502  \u2502  \u251c\u2500 Span: ClaimCheckPattern.process_publish [45ms]\n\u2502  \u2502  \u2502  \u251c\u2500 Span: s3.put_object [42ms]\n\u2502  \u2502  \u2502  \u2514\u2500 Span: generate_claim_check_id [0.1ms]\n\u2502  \u2502  \u251c\u2500 Span: db.insert_outbox [2ms]\n\u2502  \u2502  \u2514\u2500 Span: db.commit_transaction [1ms]\n\u2502  \u2514\u2500 Span: kafka.produce [3ms]\n"})}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsx)(n.h3,{id:"related-rfcs",children:"Related RFCs"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"RFC-001: Prism Architecture - Defines interface layers"}),"\n",(0,s.jsx)(n.li,{children:"RFC-002: Data Layer Interface - Client API specifications"}),"\n",(0,s.jsx)(n.li,{children:"RFC-007: Cache Strategies - Cache as a pattern"}),"\n",(0,s.jsx)(n.li,{children:"RFC-009: Distributed Reliability Patterns - Individual patterns"}),"\n",(0,s.jsx)(n.li,{children:"RFC-010: Admin Protocol with OIDC - Pattern configuration"}),"\n",(0,s.jsx)(n.li,{children:"RFC-011: Data Proxy Authentication - Authentication layer"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"external-references",children:"External References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.enterpriseintegrationpatterns.com/",children:"Enterprise Integration Patterns"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.enterpriseintegrationpatterns.com/patterns/messaging/StoreInLibrary.html",children:"Claim Check Pattern"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://microservices.io/patterns/data/transactional-outbox.html",children:"Transactional Outbox"})}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Decorator_pattern",children:"Decorator Pattern"})," - Inspiration for pattern composition"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"adrs",children:"ADRs"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"ADR-024: Layered Interface Hierarchy"}),"\n",(0,s.jsx)(n.li,{children:"ADR-025: Container Plugin Model"}),"\n",(0,s.jsx)(n.li,{children:"ADR-034: Product/Feature Sharding Strategy"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"open-questions",children:"Open Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pattern Hot-Reload"}),": Can patterns be added/removed without restarting proxy?"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pattern Configuration Evolution"}),": How to update pattern config for existing namespaces?"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pattern Performance Profiling"}),": Which patterns add most latency in production?"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Custom Patterns"}),": Can users define custom patterns via plugins?"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pattern Versioning"}),": How to version patterns independently of proxy?"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"revision-history",children:"Revision History"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"2025-10-09: Initial draft defining layered data access patterns, pattern composition, and separation of client API from backend strategies"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,n,a)=>{a.d(n,{R:()=>i,x:()=>c});var t=a(96540);const s={},r=t.createContext(s);function i(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);