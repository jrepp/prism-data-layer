"use strict";(globalThis.webpackChunkdocusaurus=globalThis.webpackChunkdocusaurus||[]).push([[1641],{28453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var a=t(96540);const o={},i=a.createContext(o);function s(n){const e=a.useContext(i);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),a.createElement(i.Provider,{value:e},n.children)}},93025:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>h,contentTitle:()=>r,default:()=>c,frontMatter:()=>s,metadata:()=>a,toc:()=>u});const a=JSON.parse('{"id":"netflix-video1","title":"Data Abstractions at Scale (Video Transcript)","description":"This is a raw transcript from a conference talk. Content may be unformatted.","source":"@site/../docs-cms/netflix/video1.md","sourceDirName":".","slug":"/netflix-video1","permalink":"/prism-data-layer/netflix/netflix-video1","draft":false,"unlisted":false,"editUrl":"https://github.com/jrepp/prism-data-layer/tree/main/docs-cms/../docs-cms/netflix/video1.md","tags":[{"inline":true,"label":"netflix","permalink":"/prism-data-layer/netflix/tags/netflix"},{"inline":true,"label":"video","permalink":"/prism-data-layer/netflix/tags/video"},{"inline":true,"label":"transcript","permalink":"/prism-data-layer/netflix/tags/transcript"},{"inline":true,"label":"abstractions","permalink":"/prism-data-layer/netflix/tags/abstractions"}],"version":"current","sidebarPosition":98,"frontMatter":{"id":"netflix-video1","title":"Data Abstractions at Scale (Video Transcript)","sidebar_label":"Video: Data Abstractions","sidebar_position":98,"tags":["netflix","video","transcript","abstractions"]},"sidebar":"netflixSidebar","previous":{"title":"Dual-Write Migration","permalink":"/prism-data-layer/netflix/netflix-dual-write-migration"},"next":{"title":"Video: Real-Time Graph","permalink":"/prism-data-layer/netflix/netflix-video2"}}');var o=t(74848),i=t(28453);const s={id:"netflix-video1",title:"Data Abstractions at Scale (Video Transcript)",sidebar_label:"Video: Data Abstractions",sidebar_position:98,tags:["netflix","video","transcript","abstractions"]},r=void 0,h={},u=[];function l(n){const e={admonition:"admonition",p:"p",...(0,i.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.admonition,{type:"warning",children:(0,o.jsx)(e.p,{children:"This is a raw transcript from a conference talk. Content may be unformatted."})}),"\n",(0,o.jsx)(e.p,{children:"This video explains how Netflix uses data abstraction layers to efficiently scale its applications and manage vast amounts of data across various use cases."}),"\n",(0,o.jsx)(e.p,{children:"The speaker, Vidya Arind, a staff engineer at Netflix, discusses:"}),"\n",(0,o.jsx)(e.p,{children:"The problem: Thousands of applications needing to interact with different storage engines, leading to complexity, varied APIs, and isolation issues (1:31).\nThe solution: Data Abstraction Layers: Introducing an extra layer of interaction between client applications and storage engines to simplify operations and provide a common interface (2:07).\nThree key concepts:\nVirtualization: Breaking down complex systems, defining clear boundaries, and being able to switch or compose implementations (2:41). This includes sharding for isolation (4:00), composition to build complex abstractions (4:25), and configuration to deploy these compositions (6:05).\nAbstraction: Making the system storage agnostic by providing a unified API (e.g., put, get, scan, delete) to clients, regardless of the underlying database (10:30). It also helps ease data migrations through shadow writing (13:36).\nClean APIs: Ensuring that the client-facing API is simple and consistent, abstracting away the underlying complexities (15:02). The video provides an example of a key-value abstraction as a two-level hashmap (15:46).\nFor the full transcript, please check the video's description!"}),"\n",(0,o.jsx)(e.p,{children:"0:07\nI have a lot to cover I'm going to\n0:09\nBreeze through this I have like 70\n0:11\nslides uh\n0:14\nlike really bad okay how to efficiently\n0:18\nscale when there are thousands of\n0:20\napplications Netflix looks like this uh\n0:23\nwe stream everywhere throughout the\n0:25\nworld wherever the those red marks are\n0:27\nuh except two places you can see that in\n0:29\nGray uh when that's the scale that\n0:31\nyou're\n0:32\nstreaming that's a lot of data I'm Vidya\n0:36\nVidya arind um I'm sta stuffer engineer\n0:38\nat data platform at Netflix and a\n0:40\nfounding member of data abstractions now\n0:42\nyou know why I want to talk about data\n0:44\nabstractions can we scale for all our\n0:47\ndata use cases that's the question um\n0:50\nwhen especially our use cases looks like\n0:53\nthis uh you have key value uh use cases\n0:56\nyour time series use cases analytics use\n0:58\nsearch use cases some times key value\n1:01\nhas larger payloads which becomes file\n1:03\nsystem or blob blob store use cases\n1:05\nright and sometimes there are no\n1:07\nSolutions Netflix is everywhere um uh\n1:10\nuse cases are everywhere in Netflix like\n1:12\nthis and Netflix also has no solutions\n1:15\nfor some of the use cases\n1:17\nright can we take these common patterns\n1:21\nand provide a common\n1:23\nsolution can these Solutions be generic\n1:26\nand storage\n1:28\nagnostic\n1:31\nour applications uh if we don't have uh\n1:34\nabstractions looks like this every\n1:36\napplication needs to understand how the\n1:38\nstorage engine operates apis are uh\n1:41\nstorage engine apis are different right\n1:44\nuh every application connects to the\n1:45\nstorage engines and has some common\n1:48\ninformation like uh it has different\n1:50\nlanguages it has different rough edges\n1:53\nand tuning parameters and cost model is\n1:55\ntotally different from all of these\n1:57\ndatabases uh your application is Con\n2:00\ninto this databases is there any\n2:01\nisolation that you're building every\n2:03\napplication has to build their own\n2:04\nisolation layer as well the\n2:07\nsolution for me is data abstraction\n2:10\nlayers right um David Willer um wheeler\n2:14\nuh rightly told we have we can solve any\n2:17\nproblem by introducing an extra layer of\n2:20\ninteraction um data in in distributed\n2:24\nsystems there are very few good ideas\n2:26\nright obstruction and virtualization can\n2:28\nbe thought about as to um uh few of\n2:31\nthose great ideas um take a complex\n2:33\nsystem uh break it down into smaller\n2:35\npieces and clearly Define the boundaries\n2:39\nthat's abstraction it take all of these\n2:41\nabstractions and uh switch the\n2:43\nimplementations or layer it or compose\n2:46\nit together that's\n2:48\nvirtualization uh here we are adding\n2:50\nabstraction layer in front of uh the\n2:53\ndatabases and in um uh in front of uh\n2:57\nafter you uh from the client applic\n3:00\nyou're connecting to a obstruction layer\n3:02\nthat's a level of interaction you're\n3:04\ntaking um obstruction uh has three main\n3:07\ncomponents one is obstruction server\n3:08\nitself it has a client which sits in\n3:11\nyour client application and it and it\n3:13\nalso has a control plane um operation\n3:16\nwhere you're you're abstracting out how\n3:18\nyou're connecting to what database using\n3:20\nwhat right um I'm going to talk today\n3:23\nabout three uh important Concepts uh\n3:25\nvirtualization obstruction and clean\n3:27\napis how do how do we do uh do these uh\n3:30\nvirtualization also has three main\n3:31\nConcepts that I want to talk about\n3:32\ncharting composition and\n3:35\nconfiguration right um uh when uh all\n3:38\nthese application like thousands of them\n3:40\nare connecting to a single abstraction\n3:43\nlayer that can be a single point of\n3:44\nfailure you need to talk uh think about\n3:47\num how how do you deploy these so that\n3:50\nwe can avoid Noisy Neighbor problems we\n3:53\ntalked about rate limiting before so\n3:55\nthat's also a way of um thinking uh\n3:59\nthinking about\n4:00\nisolation um sharding um here if you see\n4:04\nevery uh set of application is\n4:06\nconnecting to its own obstruction Ser\n4:09\nserver right obstruction layer and then\n4:12\nand the obstruction layer using the\n4:14\ncontrol plane um knows how to uh talk to\n4:17\nwhich database and when to talk to these\n4:19\ndatabases that's the isolation we are\n4:21\nproviding by charting um that's charting\n4:25\ncomposition um think about abstraction\n4:27\nas not one one simple server um here I'm\n4:31\nrepresenting key value obstruction where\n4:34\nit's a proxy uh is a it's a easy to box\n4:37\nand inside the in box you have a proxy\n4:39\nand abstraction code itself um you can\n4:43\ncomplicate that by adding a another\n4:45\nlayer this is a tree obstruction for us\n4:47\ntree obstruction is just nodes and edges\n4:50\nyou can uh think about path enumeration\n4:52\ntechnique uh uh as as your obstruction\n4:55\nand the path ination technique can be uh\n4:58\nstored in key value abstraction itself\n5:01\num you have uh custom apis given by your\n5:05\nclients that can also be part of your uh\n5:08\nabstraction layer and um UI\n5:10\npersonalization is an example here you\n5:12\ncan complicate the abstractions even\n5:15\nmore by providing something like this\n5:17\nwhere you have a front door connecting\n5:20\nto uh Journal service which is taking\n5:22\nall the requests and um kind of adding\n5:25\nAudits and uh per request um data into a\n5:29\nTime series\n5:30\nuh table that can later be used for um\n5:34\nuh correcting some of the data if there\n5:36\nis discrepancies in data as well and\n5:38\nthere's storage engine which is\n5:39\nconnecting to key value abstraction\n5:41\nother databases and search engine as\n5:44\nwell you can also use abstractions for\n5:47\nuh Shadow writing um when you have\n5:50\nmigrations to do in this case you have a\n5:53\num Thrift um container and a cql\n5:56\ncontainer connecting to different\n5:57\ndatabases and um my we'll talk about\n6:00\nmigrations a little bit later that's\n6:02\ncomposition all of this composition\n6:05\nneeds some kind of a configuration that\n6:08\nthat needs to be plugged in\n6:11\nright um You can write those\n6:13\nconfiguration down and Define how you\n6:16\nwant to compose these things right use\n6:18\nconfiguration to deploy is the next\n6:21\ntopic uh there I'm going to talk about\n6:23\ntwo configurations here one is the\n6:25\nruntime configuration where you're uh\n6:28\nliterally saying how do you compose\n6:30\nthese uh different um abstractions\n6:32\ntogether here I have key value\n6:34\nabstraction um you have uh key value\n6:38\nobstructions but in the two flavors uh\n6:40\nit's uh you have an expression and the\n6:42\nscope which defines uh which\n6:44\nconfiguration it'll use to connect to\n6:46\nwhich database right uh there is a\n6:48\nthrift um container and a KV container\n6:51\num it's wired through K Thrift as a\n6:54\nprimary and KV as a um uh shadow shadow\n6:59\nright and reads right um and you can see\n7:02\non your right that uh uh this\n7:05\nconfiguration is translated into what is\n7:08\ndeployed in your\n7:09\nright um namespace is another concept\n7:13\nthat I want to talk about namespace is\n7:14\nan abstraction on top of uh what storage\n7:19\ninin you're using it's a basic um\n7:21\nobstruction concept where Nam space is\n7:24\nuh a string that you define um and you\n7:27\nhave configuration for each of these\n7:29\nnames spaces uh in here I have version\n7:32\nand scope you can also see um like\n7:35\nphysical storage uh this uh namespace is\n7:38\nconnecting to Cassandra or evach um and\n7:42\nyou have Cassandra and evach\n7:44\nconfiguration uh in place you can also\n7:46\nstore uh consistency scope and target\n7:49\nfor the each of these um conf each of\n7:51\nthese data stores like for example read\n7:54\nyour right here uh would translate for\n7:56\nCassandra into a local quorum uh reads\n8:00\nand writes right so uh you can abstract\n8:03\nout all the information uh from the\n8:06\nclient is what uh the namespace provides\n8:09\nus um exactly so uh uh this is a watch\n8:13\nnamespace is a control plane API uh\n8:16\ngiven a Shard identity it'll return the\n8:18\nlist of name spaces um control plane is\n8:21\nwhat is talking to the obstruction\n8:23\nserver and giving us um uh all that we\n8:26\nneed to know which database to connect\n8:29\nto\n8:30\nthe control plane itself can be an\n8:32\nabstraction right like it's just uh at\n8:35\nthe end of the day how we deploy how we\n8:37\nwrite it down so control plane here is\n8:40\nuh is deployed as a abstraction control\n8:43\nplane client lives in the obstruction\n8:45\nservers instead of the client\n8:47\napplications and uh that's the\n8:49\ndifference it's talking to a uh config\n8:52\nstore uh it uses long pooling to pull\n8:55\nfor uh new name spaces that appear into\n8:57\nthe control plane that helps us do um\n9:00\nimmediately uh create sessions and\n9:03\nprepared statements and warm up your\n9:06\nqueries um before the obstruction starts\n9:09\nor when the new name spaces are\n9:12\nadded uh we we do a bunch of things with\n9:15\nuh control plane itself like um you use\n9:17\ntemporal workflows or spinco pipelines\n9:20\nand uh use Python code a little bit to\n9:24\ndeploy or create new name spaces and all\n9:27\nthe artifacts related to the namespaces\n9:29\nfor examp example tables and clusters\n9:31\nand things like that um watch name space\n9:34\nthis is an important API for control\n9:36\nplane um watch names space uh I'm good\n9:39\non time okay watch Nam space uh has uh\n9:43\ntakes in uh short identity and the last\n9:45\nseen version and if there's a new\n9:47\nversion it returns back um uh the list\n9:50\nof name spaces uh with with the version\n9:53\nthat it sees U clone namespace is a\n9:56\nlittle bit different concept where you\n9:58\nare taking a conf configuration of a\n9:59\nsource namespace and creating a target\n10:02\nnamespace with a new artifacts like new\n10:04\ncandra cluster or um a new table in\n10:08\ninside a Centra cluster it it's a\n10:10\nasynchronous process where you are\n10:13\nwaiting for things to be done that's why\n10:15\nyou get a job ID back that um concludes\n10:19\nmy virtualization it's just not limited\n10:21\nto what I talked about but there's more\n10:23\nyou can um derive from this um so the\n10:27\nsorry uh the next concept I want to talk\n10:30\nabout is abstraction itself um I want to\n10:33\ntalk two main things about abstraction\n10:35\nthere's tons more to talk about uh I I\n10:38\nthink given the time I think two is is a\n10:41\ngood uh good compromise um storage\n10:44\nagnostic how do you make uh obstruction\n10:47\nstorage agnostic and dual rights um uh\n10:51\neverybody here has done migration at\n10:53\nsome point I'm I'm hoping um the main\n10:57\nobstruction I'm going to talk about is\n10:58\nkey value obstruction here um uh David\n11:01\nrightly pointed out API is your contract\n11:04\nwith a CL uh client that you have to\n11:07\nsolidify your apis what you do in the\n11:10\nback end is just abstracted out from the\n11:13\ncustomers right uh you can see your put\n11:16\nget scan delete is the contract that you\n11:18\nare giving to the customers which is the\n11:20\nclient applications what happens uh how\n11:23\nwe call the underlying apis of each and\n11:27\nevery database is agnos here right um uh\n11:31\nin this case when I talk to mcash D um I\n11:34\nuse sets and gets in Cassandra I use\n11:37\nselects and inserts and Dynamo DB I use\n11:40\num put put and\n11:42\nqueries uh in obstruction layer itself\n11:45\nwe have different data stores or record\n11:47\nstores which uh which helps us um\n11:50\nabstract out that details and control\n11:52\nplane has the information about which\n11:54\ndatabase to connect to when the request\n11:56\ncomes in for a specific name space um uh\n11:59\nand depending on the record store that\n12:01\nthe control plane dictates uh you\n12:03\nconnect to that particular database um\n12:06\nwith the record store\n12:07\ninformation uh again it's a namespace\n12:10\none here in your left has a see uh it's\n12:13\nconnecting to a Cassandra database and\n12:15\nnamespace 2 here um connects to the\n12:17\nDynamo DB database is just how it's\n12:20\nconfigured right uh the request comes in\n12:22\nwith the namespace name that's how you\n12:24\nknow where to connect\n12:25\nto that makes it storage agnostic right\n12:29\num as I said everybody almost I think\n12:33\nwould have gone through migrations and\n12:34\nmigrations are painful right um how can\n12:38\nabstraction help ease this Spain uh last\n12:40\nyear we ran a program for uh convert uh\n12:44\nmoving from Thrift to cql at Netflix it\n12:47\ntook almost a a year and a half for us\n12:50\nto migrate like 250 Cassandra clusters\n12:53\nfrom three uh 2 Cassandra to 30\n12:56\nCassandra right um that\n13:00\nplus some use cases like these right um\n13:04\nwhere uh user comes to us um and says I\n13:09\nI only have a gigabyte of data and\n13:12\nquickly in a year or so realizes oh I I\n13:15\nI have to store more data Json BL uh I\n13:18\nwant to store a larger Json blob I I I\n13:21\nwant to uh store just not use the\n13:24\ndatabase I have right uh all of these\n13:27\nrequires migration uh use case starts as\n13:30\na simple key value quickly moves into a\n13:33\nblob\n13:34\nstore we need to\n13:36\nmigrate migration for us looks like this\n13:39\nright um uh the client API is always the\n13:42\nkey value API um or whatever the\n13:45\nobstruction they're using um we migrate\n13:48\num uh we start like this DB1 is your one\n13:51\nimplementation um we add db2 which is\n13:55\nour sha which is in the shadow right\n13:57\nmode I talked about Shadow right earlier\n13:59\nwe now are talking to both the databases\n14:02\nparall um uh underneath we move the data\n14:06\nfrom DB1 to db2 backfilling the data\n14:09\nfrom DB1 to D db2 all of this happens\n14:12\nwithout client even touching anything in\n14:14\ntheir site and we promote db2 as a\n14:17\nprimary and DB1 is still getting the\n14:19\ndata but it's uh is just in the shadow\n14:23\nmode um and then we go on decommission\n14:26\nDB1 at that point all the trace of old\n14:30\nuh databases gone right um uh persistent\n14:34\nconfig for dual rights looks something\n14:36\nlike this um you have uh the same\n14:39\nnamespace names space one um having two\n14:42\npersistent config we talked about SC\n14:44\nscope a little bit earlier I probably\n14:46\nBreeze through it faster scope is how we\n14:49\nare um uh telling the\n14:51\ncontainer uh scope this configuration to\n14:54\nthat particular\n14:56\ncontainer great uh that's up obstruction\n14:59\nfor us um all this is coming to is a\n15:02\nclean API uh no matter how many\n15:05\nobstructions you have you have to have a\n15:07\nclean and simple API in the client's\n15:09\nside so that they come and use your\n15:11\nabstractions one and uh it's simpler for\n15:15\nthem to move and they're not aware of\n15:17\nall the um Oddities that are happening\n15:19\nin the end right um for key value\n15:21\nobstruction that I'm going to talk about\n15:23\ntoday uh simple API like you can you can\n15:27\nalmost think of this as like a Java apis\n15:29\nright there's no almost no difference\n15:31\nbetween puts gets um gets mutate mutate\n15:35\nis little different but M\n15:37\nmutates um scan put a faps and or\n15:40\ncompute those are the simple apis um to\n15:44\nuh think about uh key value obstruction\n15:46\nthink about it as uh two level hashmap\n15:50\nright um one level is your IDs and the\n15:52\nsecond level is a sorted map of bytes\n15:55\nand\n15:56\nbytes uh so uh key Val storage layer\n16:00\nlooks uh the base table looks like this\n16:02\nwhere when when you have a simple\n16:03\npayload it's ID and key key is a\n16:06\nclustering column which is the sorted\n16:08\nsorted map I talked about and the value\n16:10\num I have I I'll come back to the value\n16:13\nmetadata in a bit item itself looks very\n16:16\nsimple bytes right a key key is a bite\n16:19\nvalues a bite and a some uh value met\n16:22\ndata if\n16:23\nthe value itself is a large uh value\n16:26\nthen you uh you need chunking uh chunk\n16:28\nchunk information as well metadata is a\n16:32\nlittle complicated uh concept uh but uh\n16:35\nthere are other talks you can go listen\n16:36\nto about metadata um one is U\n16:40\ncompression you want to do client side\n16:42\ncompression and when you do client side\n16:44\ncompression you need to store which\n16:46\nalgorithm did you use to compress the\n16:47\ndata kind of information so that's uh uh\n16:51\ncompression life cycle right times\n16:53\nexpired times uh Etc can be stored in\n16:56\nlife cycle metadata chunk metadata where\n16:59\nis the chunk how many chunks did you\n17:01\nmake of the payload um where is it\n17:03\nstored and what hash algorithms did you\n17:05\nuse to chunk all of that is stored in\n17:08\nchunk metadata content metadata is how\n17:10\nyou're rendering that uh data back to\n17:13\nthe client that's uh metadata put calls\n17:15\nvery simple you have a namespace here uh\n17:19\nyou can see how the requests are coming\n17:21\nthrough to the service through uh simple\n17:24\nnamespace as the abstraction uh part\n17:27\nright like using a namespace you can\n17:29\nagain find out who who call which uh\n17:33\nservice to call or which database to\n17:35\ncall um ID and a list of items um item\n17:38\npoy token is very interesting um I uh it\n17:41\nis generated in the client side for us\n17:44\nuh client side generates a monotonically\n17:45\nincreasing item proty token and attaches\n17:48\nit to every put call and get calls um\n17:52\nmutation is list of puts and gets uh we\n17:55\norder these puts and gets in uh in the\n17:57\nsame mutation request using the item\n17:59\nPary token again we have item token.\n18:01\nnext which will give you a monotonically\n18:04\nincreasing number um it's interesting uh\n18:07\nget items is given a name space and an\n18:10\nID with the predicate which which uh\n18:13\ndictates match all um match a list of\n18:16\nkeys or uh match a range of queries um\n18:21\nit'll return a list of items and the\n18:22\nnext page token which will help you Pate\n18:25\nthrough the whole list scan uh this\n18:28\nagain is uh very interesting for\n18:30\nmigrations um scan uh given a name space\n18:33\nwe want to scan the whole uh whole table\n18:36\nor whole name space with all the data\n18:38\nlike it can have have hundreds or in a\n18:41\nbillion and you want to paginate those\n18:43\ndata so you what is returned is a scan\n18:45\nresult with the next page token where\n18:48\nyou p through the tokens um I want to uh\n18:53\ngo through this but let's let's see if I\n18:54\ncan make it I have a small amount of\n18:58\ntime if the payload is small less than 1\n19:00\nMB there is nothing you can uh you need\n19:03\nto do just store it straight to the\n19:05\ndatabase right what else um if your\n19:08\npayload is large like 4 MB then you have\n19:11\nto chunk the data when you chunk the\n19:13\ndata you chunk it into 64 KB um of\n19:17\nchunks and you stream the chunks into\n19:19\nthe server or commit it to the server\n19:22\nafter you commit you commit chunk zero\n19:25\nchunk zero is what is determining have\n19:28\nyou done the all the work that is needed\n19:30\nto so that the um the data is visible to\n19:33\nthe customer right um in the read path\n19:36\nyou first read chunk zero determine\n19:38\nwhere your chunks are and then go fetch\n19:41\nparallely all the\n19:42\nchunks and um Stitch it together the\n19:45\nchunks for us lives in a data table um\n19:48\nwhen when you uh determine It's a larger\n19:51\npayload you uh store the value metadata\n19:53\nin the base table and value is empty and\n19:56\nvalue metadata determines where the data\n19:58\nis um we uh spread the load of uh\n20:02\ndifferent chunks using bucketing um\n20:04\nbucketing strategy ID is your primary\n20:07\nstill uh you bucket the data and your\n20:09\nkey is uh sorted uh key chunk and\n20:12\nversion ID are uh the clustering columns\n20:16\nright um okay uh I am I am going to be\n20:19\ndone in one minute okay\n20:23\nuh so this is how you spread the load um\n20:26\nI'm going to I think that con includes\n20:29\nhow my API uh looks there are a lot of\n20:32\nbuilding blocks you can see for key\n20:34\nvalue abstraction we have done chunking\n20:37\ncompression adaptive pagination caching\n20:40\nsignaling SLO signaling summarization\n20:42\nthere are tons of features you add and\n20:46\nabstraction with many tunable features\n20:48\nis what I want to leave you with uh more\n20:52\nabstractions yeah this is just a general\n20:54\nconcept right like you can build many\n20:56\nabstractions with that you can see\n20:59\nuh there are tons that we have built and\n21:02\nwe are adding more as well so for\n21:04\nexample uh time uh uh uh key values are\n21:08\noldest obstruction with 400 charts in it\n21:11\nuh time series counter identifier entity\n21:14\ntree graph Q you can keep going I'll\n21:18\nleave you with Mark Anderson's code\n21:20\nevery new layer of obstruction is a new\n21:22\nchance for a clean slate redesign of\n21:24\neverything making everything little\n21:26\nfaster less power hungry more elegant\n21:29\neasy to use and\n21:31\ncheaper thank\n21:33\n[Applause]\n21:42\nyou"})]})}function c(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(l,{...n})}):l(n)}}}]);