"use strict";(globalThis.webpackChunkdocusaurus=globalThis.webpackChunkdocusaurus||[]).push([[6864],{27795:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"rfc-005","title":"ClickHouse Integration for Time Series Analytics","description":"Abstract","source":"@site/../docs-cms/rfcs/rfc-005-clickhouse-integration.md","sourceDirName":".","slug":"/rfc-005","permalink":"/prism-data-layer/rfc/rfc-005","draft":false,"unlisted":false,"editUrl":"https://github.com/jrepp/prism-data-layer/tree/main/docs-cms/../docs-cms/rfcs/rfc-005-clickhouse-integration.md","tags":[],"version":"current","frontMatter":{"author":"Core Team","created":"2025-10-08T00:00:00.000Z","doc_uuid":"a54be3cc-e916-4748-8a8b-20fdabf89a94","id":"rfc-005","project_id":"prism-data-layer","related":"RFC-001, RFC-002, RFC-004","status":"Proposed","title":"ClickHouse Integration for Time Series Analytics","updated":"2025-10-08T00:00:00.000Z"},"sidebar":"rfcSidebar","previous":{"title":"Redis Integration for Cache, PubSub, and Vector Search \u2022 RFC-004","permalink":"/prism-data-layer/rfc/rfc-004"},"next":{"title":"RFC-006 Admin CLI","permalink":"/prism-data-layer/rfc/rfc-006"}}');var i=s(74848),r=s(28453);const a={author:"Core Team",created:new Date("2025-10-08T00:00:00.000Z"),doc_uuid:"a54be3cc-e916-4748-8a8b-20fdabf89a94",id:"rfc-005",project_id:"prism-data-layer",related:"RFC-001, RFC-002, RFC-004",status:"Proposed",title:"ClickHouse Integration for Time Series Analytics",updated:new Date("2025-10-08T00:00:00.000Z")},o=void 0,l={},c=[{value:"Abstract",id:"abstract",level:2},{value:"1. Introduction",id:"1-introduction",level:2},{value:"1.1 Purpose",id:"11-purpose",level:3},{value:"1.2 Goals",id:"12-goals",level:3},{value:"1.3 Non-Goals",id:"13-non-goals",level:3},{value:"2. Architecture Overview",id:"2-architecture-overview",level:2},{value:"2.1 Time Series Pipeline",id:"21-time-series-pipeline",level:3},{value:"2.2 Data Flow",id:"22-data-flow",level:3},{value:"3. Time Series Access Pattern",id:"3-time-series-access-pattern",level:2},{value:"3.1 Use Cases",id:"31-use-cases",level:3},{value:"3.2 Interface",id:"32-interface",level:3},{value:"3.3 Schema Design",id:"33-schema-design",level:3},{value:"3.4 Materialized Views for Pre-aggregations",id:"34-materialized-views-for-pre-aggregations",level:3},{value:"4. Query Patterns",id:"4-query-patterns",level:2},{value:"4.1 Time Range Queries",id:"41-time-range-queries",level:3},{value:"4.2 Aggregations",id:"42-aggregations",level:3},{value:"4.3 Top-N Queries",id:"43-top-n-queries",level:3},{value:"4.4 Time Series Bucketing",id:"44-time-series-bucketing",level:3},{value:"5. Performance Optimizations",id:"5-performance-optimizations",level:2},{value:"5.1 Partitioning Strategy",id:"51-partitioning-strategy",level:3},{value:"5.2 Compression",id:"52-compression",level:3},{value:"5.3 Async Inserts",id:"53-async-inserts",level:3},{value:"6. Configuration",id:"6-configuration",level:2},{value:"6.1 Client Configuration",id:"61-client-configuration",level:3},{value:"6.2 Server Configuration",id:"62-server-configuration",level:3},{value:"7. Operational Considerations",id:"7-operational-considerations",level:2},{value:"7.1 Capacity Planning",id:"71-capacity-planning",level:3}];function d(e){const n={br:"br",code:"code",h2:"h2",h3:"h3",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"abstract",children:"Abstract"}),"\n",(0,i.jsx)(n.p,{children:"This RFC specifies the integration of ClickHouse into Prism as a high-performance OLAP database optimized for time series analytics. ClickHouse provides columnar storage, real-time ingestion, and lightning-fast analytical queries, making it ideal for metrics, logs, events, and observability data."}),"\n",(0,i.jsx)(n.h2,{id:"1-introduction",children:"1. Introduction"}),"\n",(0,i.jsx)(n.h3,{id:"11-purpose",children:"1.1 Purpose"}),"\n",(0,i.jsx)(n.p,{children:"ClickHouse integration addresses analytical time series workloads:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Metrics Storage"}),": Application metrics, system metrics, business KPIs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Event Logging"}),": Application logs, audit logs, user activity events"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Observability"}),": Traces, spans, and distributed system telemetry"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Analytics"}),": Real-time aggregations, rollups, and dashboards"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"12-goals",children:"1.2 Goals"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Ingestion Rate"}),": 1M+ events/sec sustained write throughput"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Query Performance"}),": Sub-second aggregations over billions of rows"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Compression"}),": 10-100x compression ratio for time series data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Retention"}),": Automatic data lifecycle with TTL and tiered storage"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scalability"}),": Horizontal scaling with ReplicatedMergeTree"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"13-non-goals",children:"1.3 Non-Goals"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Not for OLTP"}),": Use Postgres for transactional workloads"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Not for full-text search"}),": Use Elasticsearch or TypeSense"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Not for updates"}),": ClickHouse is append-only (use Postgres for mutable data)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Not for joins"}),": Optimized for denormalized data, avoid complex joins"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"2-architecture-overview",children:"2. Architecture Overview"}),"\n",(0,i.jsx)(n.h3,{id:"21-time-series-pipeline",children:"2.1 Time Series Pipeline"}),"\n",(0,i.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Ingestion"\n        Client[Client Applications]\n        Proxy[Prism Proxy]\n        Buffer[Async Buffer<br/>Batching]\n    end\n\n    subgraph "ClickHouse Cluster"\n        Shard1[Shard 1<br/>ReplicatedMergeTree]\n        Shard2[Shard 2<br/>ReplicatedMergeTree]\n        Replica1[Replica 1]\n        Replica2[Replica 2]\n    end\n\n    subgraph "Query"\n        Query[Query Service]\n        Materialized[Materialized Views<br/>Pre-aggregations]\n    end\n\n    Client --\x3e|Write Batch| Proxy\n    Proxy --\x3e Buffer\n    Buffer --\x3e|Distributed Write| Shard1\n    Buffer --\x3e|Distributed Write| Shard2\n\n    Shard1 -.->|Replication| Replica1\n    Shard2 -.->|Replication| Replica2\n\n    Client --\x3e|Analytical Query| Proxy\n    Proxy --\x3e Query\n    Query --\x3e Materialized\n    Materialized --\x3e Shard1\n    Materialized --\x3e Shard2'}),"\n",(0,i.jsx)(n.h3,{id:"22-data-flow",children:"2.2 Data Flow"}),"\n",(0,i.jsx)(n.mermaid,{value:"sequenceDiagram\n    participant App as Application\n    participant Proxy as Prism Proxy\n    participant Buffer as Async Buffer\n    participant CH as ClickHouse\n\n    App->>Proxy: AppendEvents([event1, event2, ...])\n    Proxy->>Buffer: Queue events\n    Proxy--\x3e>App: AppendResponse{queued: 1000}\n\n    Note over Buffer: Batch accumulation<br/>(1000 events or 1s)\n\n    Buffer->>CH: INSERT INTO events VALUES (...)\n    CH--\x3e>Buffer: OK (1000 rows)"}),"\n",(0,i.jsx)(n.h2,{id:"3-time-series-access-pattern",children:"3. Time Series Access Pattern"}),"\n",(0,i.jsx)(n.h3,{id:"31-use-cases",children:"3.1 Use Cases"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Metrics"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Application performance metrics (request rate, latency, errors)"}),"\n",(0,i.jsx)(n.li,{children:"Infrastructure metrics (CPU, memory, disk, network)"}),"\n",(0,i.jsx)(n.li,{children:"Business metrics (revenue, conversions, user activity)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Logs"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Application logs (errors, warnings, debug)"}),"\n",(0,i.jsx)(n.li,{children:"Access logs (HTTP requests, API calls)"}),"\n",(0,i.jsx)(n.li,{children:"Audit logs (security events, compliance)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Events"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"User activity (clicks, page views, sessions)"}),"\n",(0,i.jsx)(n.li,{children:"System events (deployments, configuration changes)"}),"\n",(0,i.jsx)(n.li,{children:"IoT telemetry (sensor data, device events)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Traces"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Distributed tracing spans"}),"\n",(0,i.jsx)(n.li,{children:"Service dependencies and call graphs"}),"\n",(0,i.jsx)(n.li,{children:"Performance profiling"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"32-interface",children:"3.2 Interface"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-protobuf",children:'syntax = "proto3";\n\npackage prism.timeseries.v1;\n\nservice TimeSeriesService {\n  // Append events (batched, async)\n  rpc AppendEvents(AppendEventsRequest) returns (AppendEventsResponse);\n\n  // Stream events (for high-throughput ingestion)\n  rpc StreamEvents(stream Event) returns (StreamEventsResponse);\n\n  // Query events with time range and filters\n  rpc QueryEvents(QueryRequest) returns (QueryResponse);\n\n  // Query with aggregations\n  rpc QueryAggregates(AggregateRequest) returns (AggregateResponse);\n\n  // Stream query results (for large datasets)\n  rpc StreamQuery(QueryRequest) returns (stream Event);\n}\n\nmessage Event {\n  // Timestamp (nanosecond precision)\n  google.protobuf.Timestamp timestamp = 1;\n\n  // Event metadata\n  string event_type = 2;\n  string source = 3;\n\n  // Dimensions (indexed)\n  map<string, string> dimensions = 4;\n\n  // Metrics (columnar storage)\n  map<string, double> metrics = 5;\n\n  // Payload (compressed)\n  bytes payload = 6;\n}\n\nmessage AppendEventsRequest {\n  string session_id = 1;\n  string namespace = 2;\n  repeated Event events = 3;\n\n  // Async mode (immediate response)\n  bool async = 4;\n}\n\nmessage QueryRequest {\n  string session_id = 1;\n  string namespace = 2;\n\n  // Time range (required for partition pruning)\n  google.protobuf.Timestamp start_time = 3;\n  google.protobuf.Timestamp end_time = 4;\n\n  // Filters (WHERE clause)\n  map<string, string> filters = 5;\n\n  // SQL-like query (optional)\n  string sql = 6;\n\n  // Pagination\n  int32 limit = 7;\n  int32 offset = 8;\n}\n\nmessage AggregateRequest {\n  string session_id = 1;\n  string namespace = 2;\n\n  google.protobuf.Timestamp start_time = 3;\n  google.protobuf.Timestamp end_time = 4;\n\n  // Aggregation function\n  enum AggregateFunc {\n    COUNT = 0;\n    SUM = 1;\n    AVG = 2;\n    MIN = 3;\n    MAX = 4;\n    QUANTILE_95 = 5;\n    QUANTILE_99 = 6;\n  }\n\n  // Metrics to aggregate\n  repeated string metric_names = 5;\n  repeated AggregateFunc functions = 6;\n\n  // Group by dimensions\n  repeated string group_by = 7;\n\n  // Time bucket (for time series)\n  google.protobuf.Duration bucket_size = 8;\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"33-schema-design",children:"3.3 Schema Design"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Main events table (sharded by timestamp)\nCREATE TABLE events ON CLUSTER '{cluster}' (\n    timestamp DateTime64(9),\n    event_type LowCardinality(String),\n    source LowCardinality(String),\n    namespace LowCardinality(String),\n\n    -- Dimensions (indexed)\n    dimensions Map(String, String),\n\n    -- Metrics (columnar)\n    metrics Map(String, Float64),\n\n    -- Payload (compressed)\n    payload String CODEC(ZSTD(3))\n)\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/events', '{replica}')\nPARTITION BY toYYYYMMDD(timestamp)\nORDER BY (namespace, event_type, timestamp)\nTTL timestamp + INTERVAL 90 DAY\nSETTINGS index_granularity = 8192;\n\n-- Distributed table (query interface)\nCREATE TABLE events_distributed ON CLUSTER '{cluster}' AS events\nENGINE = Distributed('{cluster}', default, events, rand());\n"})}),"\n",(0,i.jsx)(n.h3,{id:"34-materialized-views-for-pre-aggregations",children:"3.4 Materialized Views for Pre-aggregations"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- 1-minute rollups\nCREATE MATERIALIZED VIEW events_1m ON CLUSTER '{cluster}'\nENGINE = ReplicatedSummingMergeTree('/clickhouse/tables/{shard}/events_1m', '{replica}')\nPARTITION BY toYYYYMMDD(timestamp)\nORDER BY (namespace, event_type, timestamp_1m)\nAS SELECT\n    toStartOfMinute(timestamp) AS timestamp_1m,\n    namespace,\n    event_type,\n    count() AS event_count,\n    sumMap(metrics) AS metrics_sum,\n    avgMap(metrics) AS metrics_avg\nFROM events\nGROUP BY timestamp_1m, namespace, event_type;\n\n-- 1-hour rollups (from 1-minute)\nCREATE MATERIALIZED VIEW events_1h ON CLUSTER '{cluster}'\nENGINE = ReplicatedSummingMergeTree('/clickhouse/tables/{shard}/events_1h', '{replica}')\nPARTITION BY toYYYYMM(timestamp)\nORDER BY (namespace, event_type, timestamp_1h)\nAS SELECT\n    toStartOfHour(timestamp_1m) AS timestamp_1h,\n    namespace,\n    event_type,\n    sum(event_count) AS event_count,\n    sumMap(metrics_sum) AS metrics_sum\nFROM events_1m\nGROUP BY timestamp_1h, namespace, event_type;\n"})}),"\n",(0,i.jsx)(n.h2,{id:"4-query-patterns",children:"4. Query Patterns"}),"\n",(0,i.jsx)(n.h3,{id:"41-time-range-queries",children:"4.1 Time Range Queries"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Recent events (last 1 hour)\nSELECT *\nFROM events_distributed\nWHERE namespace = 'app:production'\n  AND timestamp >= now() - INTERVAL 1 HOUR\nORDER BY timestamp DESC\nLIMIT 100;\n"})}),"\n",(0,i.jsx)(n.h3,{id:"42-aggregations",children:"4.2 Aggregations"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Request rate per minute (last 24 hours)\nSELECT\n    toStartOfMinute(timestamp) AS minute,\n    count() AS request_count,\n    avg(metrics['duration_ms']) AS avg_duration,\n    quantile(0.95)(metrics['duration_ms']) AS p95_duration\nFROM events_distributed\nWHERE namespace = 'api:requests'\n  AND timestamp >= now() - INTERVAL 24 HOUR\nGROUP BY minute\nORDER BY minute;\n"})}),"\n",(0,i.jsx)(n.h3,{id:"43-top-n-queries",children:"4.3 Top-N Queries"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Top 10 endpoints by error rate\nSELECT\n    dimensions['endpoint'] AS endpoint,\n    countIf(dimensions['status'] >= '500') AS error_count,\n    count() AS total_count,\n    error_count / total_count AS error_rate\nFROM events_distributed\nWHERE namespace = 'api:requests'\n  AND timestamp >= now() - INTERVAL 1 HOUR\nGROUP BY endpoint\nORDER BY error_rate DESC\nLIMIT 10;\n"})}),"\n",(0,i.jsx)(n.h3,{id:"44-time-series-bucketing",children:"4.4 Time Series Bucketing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- P99 latency per 5-minute bucket\nSELECT\n    toStartOfInterval(timestamp, INTERVAL 5 MINUTE) AS bucket,\n    quantile(0.99)(metrics['latency_ms']) AS p99_latency\nFROM events_distributed\nWHERE namespace = 'service:checkout'\n  AND timestamp >= now() - INTERVAL 6 HOUR\nGROUP BY bucket\nORDER BY bucket;\n"})}),"\n",(0,i.jsx)(n.h2,{id:"5-performance-optimizations",children:"5. Performance Optimizations"}),"\n",(0,i.jsx)(n.h3,{id:"51-partitioning-strategy",children:"5.1 Partitioning Strategy"}),"\n",(0,i.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Partition by Day"\n        P1[2025-10-01<br/>Partition 1]\n        P2[2025-10-02<br/>Partition 2]\n        P3[2025-10-03<br/>Partition 3]\n    end\n\n    subgraph "Each Partition"\n        Data[Data Parts<br/>8K granules]\n        Index[Primary Index<br/>namespace + event_type]\n        Marks[Sparse Index<br/>Mark files]\n    end\n\n    P1 --\x3e Data\n    P2 --\x3e Data\n    P3 --\x3e Data\n\n    Data --\x3e Index\n    Index --\x3e Marks'}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Benefits"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Partition Pruning"}),": Only scan relevant days"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"TTL"}),": Drop old partitions efficiently"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Parallel Queries"}),": Query partitions in parallel"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"52-compression",children:"5.2 Compression"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Codec comparison\nALTER TABLE events MODIFY COLUMN payload String CODEC(ZSTD(3));  -- 10x compression\nALTER TABLE events MODIFY COLUMN dimensions Map(String, String) CODEC(LZ4);  -- 3x, faster\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Compression Ratios"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ZSTD(3)"}),": 10-15x (best compression)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LZ4"}),": 2-3x (fastest)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Delta + LZ4"}),": 20x+ for monotonic data (timestamps, counters)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"53-async-inserts",children:"5.3 Async Inserts"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Enable async inserts (client-side batching)\nSET async_insert = 1;\nSET wait_for_async_insert = 0;\nSET async_insert_max_data_size = 10000000;  -- 10MB batches\nSET async_insert_busy_timeout_ms = 1000;     -- 1s max wait\n"})}),"\n",(0,i.jsx)(n.h2,{id:"6-configuration",children:"6. Configuration"}),"\n",(0,i.jsx)(n.h3,{id:"61-client-configuration",children:"6.1 Client Configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-protobuf",children:'message ClickHouseBackendConfig {\n  // Connection\n  repeated string hosts = 1;\n  int32 port = 2;\n  string database = 3;\n  string username = 4;\n  string password = 5;\n\n  // Cluster settings\n  bool cluster_mode = 6;\n  string cluster_name = 7;\n  int32 num_shards = 8;\n  int32 num_replicas = 9;\n\n  // Performance\n  int32 batch_size = 10;             // Events per batch\n  google.protobuf.Duration batch_timeout = 11;  // Max wait time\n  bool async_insert = 12;\n  int32 insert_threads = 13;\n\n  // Retention\n  int32 ttl_days = 14;\n  bool enable_tiered_storage = 15;\n\n  // Table settings\n  string partition_by = 16;  // "toYYYYMMDD(timestamp)"\n  string order_by = 17;      // "(namespace, event_type, timestamp)"\n  int32 index_granularity = 18;\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"62-server-configuration",children:"6.2 Server Configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# config/clickhouse.yaml\nclickhouse:\n  cluster:\n    name: "prism_cluster"\n    shards:\n      - hosts: ["ch-shard1-1.internal:9000", "ch-shard1-2.internal:9000"]\n      - hosts: ["ch-shard2-1.internal:9000", "ch-shard2-2.internal:9000"]\n\n  tables:\n    events:\n      engine: "ReplicatedMergeTree"\n      partition_by: "toYYYYMMDD(timestamp)"\n      order_by: "(namespace, event_type, timestamp)"\n      ttl_days: 90\n      compression: "ZSTD(3)"\n\n  performance:\n    batch_size: 10000\n    batch_timeout: "1s"\n    async_insert: true\n    insert_threads: 4\n    max_memory_usage: "10GB"\n\n  materialized_views:\n    - name: "events_1m"\n      enabled: true\n    - name: "events_1h"\n      enabled: true\n    - name: "events_1d"\n      enabled: false  # Enable for long-term storage\n'})}),"\n",(0,i.jsx)(n.h2,{id:"7-operational-considerations",children:"7. Operational Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"71-capacity-planning",children:"7.1 Capacity Planning"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Storage Calculation"}),":\ndaily_volume = events_per_day \xd7 avg_event_size_bytes\ncompressed_size = daily_volume / compression_ratio\nretention_storage = compressed_size \xd7 retention_days \xd7 (1 + replica_count)"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"\n**Example**:\n- 1B events/day \xd7 200 bytes = 200GB/day uncompressed\n- 200GB / 10 (ZSTD) = 20GB/day compressed\n- 20GB \xd7 90 days \xd7 2 (replication) = 3.6TB total\n\n**Query Performance**:\n- 1M events/sec ingestion requires ~4 shards\n- Sub-second queries over 1TB datasets\n- 100 concurrent analytical queries supported\n\n### 7.2 Monitoring\n\n"})}),"\n",(0,i.jsx)(n.p,{children:"metrics:\ningestion:\n- insert_rate_events_per_sec\n- insert_throughput_bytes_per_sec\n- async_insert_queue_size\n- insert_latency_p99"}),"\n",(0,i.jsx)(n.p,{children:"storage:\n- disk_usage_bytes\n- compression_ratio\n- parts_count\n- partition_count"}),"\n",(0,i.jsx)(n.p,{children:"queries:\n- query_rate_per_sec\n- query_latency_p50\n- query_latency_p99\n- memory_usage_per_query\n- running_queries_count"}),"\n",(0,i.jsx)(n.p,{children:"replication:\n- replication_lag_seconds\n- replica_queue_size"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"\n### 7.3 Data Lifecycle\n\n"})}),"\n",(0,i.jsxs)(n.p,{children:["graph LR\nHot[Hot Storage",(0,i.jsx)(n.br,{}),"SSD",(0,i.jsx)(n.br,{}),"Last 7 days] --\x3e|TTL| Warm[Warm Storage",(0,i.jsx)(n.br,{}),"HDD",(0,i.jsx)(n.br,{}),"8-30 days]\nWarm --\x3e|TTL| Cold[Cold Storage",(0,i.jsx)(n.br,{}),"S3",(0,i.jsx)(n.br,{}),"31-90 days]\nCold --\x3e|TTL| Delete[Deleted",(0,i.jsx)(n.br,{}),"90+ days]"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"style Hot fill:#ff6b6b\nstyle Warm fill:#feca57\nstyle Cold fill:#48dbfb\nstyle Delete fill:#dfe6e9\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text"})}),"\n",(0,i.jsx)(n.p,{children:"-- Tiered storage with TTL\nALTER TABLE events MODIFY TTL\ntimestamp + INTERVAL 7 DAY TO VOLUME 'hot',\ntimestamp + INTERVAL 30 DAY TO VOLUME 'warm',\ntimestamp + INTERVAL 90 DAY TO VOLUME 'cold',\ntimestamp + INTERVAL 90 DAY DELETE;"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"\n## 8. Migration Path\n\n### 8.1 Phase 1: Single Node (Week 1-2)\n\n- Deploy ClickHouse standalone\n- Implement TimeSeriesService gRPC interface\n- Create basic events table\n- Integration tests with mock data\n\n### 8.2 Phase 2: Cluster Setup (Week 3-4)\n\n- Deploy 2-shard cluster with replication\n- Configure Distributed tables\n- Implement async batching\n- Load testing (1M events/sec)\n\n### 8.3 Phase 3: Materialized Views (Week 5-6)\n\n- Create 1-minute rollups\n- Create 1-hour rollups\n- Query optimization with pre-aggregations\n- Benchmarking (query latency)\n\n### 8.4 Phase 4: Production (Week 7-8)\n\n- Tiered storage configuration\n- Alerting and monitoring\n- Backup and disaster recovery\n- Documentation and runbooks\n\n## 9. Use Case Recommendations\n\n### 9.1 When to Use ClickHouse\n\n\u2705 **Use When**:\n- Append-only time series data\n- Analytical queries over large datasets\n- Real-time aggregations needed\n- High ingestion rate (`>100k` events/sec)\n- Data has natural time dimension\n\n\u274c **Avoid When**:\n- Need updates/deletes (use Postgres)\n- Complex joins required (use Postgres)\n- Full-text search needed (use Elasticsearch)\n- Small dataset (`<1M` rows, use Postgres)\n\n### 9.2 ClickHouse vs Alternatives\n\n| Use Case | ClickHouse | Postgres | Kafka |\n|----------|-----------|----------|-------|\n| Metrics storage | \u2705 Excellent | \u274c Poor | \u274c No analytics |\n| Event logging | \u2705 Excellent | \u26a0\ufe0f Limited | \u2705 Good |\n| Real-time dashboards | \u2705 Excellent | \u274c Slow | \u274c No queries |\n| Retention (90+ days) | \u2705 Efficient | \u274c Expensive | \u26a0\ufe0f Complex |\n| Complex aggregations | \u2705 Fast | \u26a0\ufe0f Moderate | \u274c Not supported |\n\n## 10. References\n\n- [ClickHouse Documentation](https://clickhouse.com/docs/)\n- [ReplicatedMergeTree Engine](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication)\n- [Materialized Views](https://clickhouse.com/docs/en/guides/developer/cascading-materialized-views)\n- [Compression Codecs](https://clickhouse.com/docs/en/sql-reference/statements/create/table#column_compression_codec)\n\n## 11. Revision History\n\n- 2025-10-08: Initial draft\n\n"})})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>o});var t=s(96540);const i={},r=t.createContext(i);function a(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);